# 李宏毅机器学习

![image-20210117112454225](C:\Users\GY\AppData\Roaming\Typora\typora-user-images\image-20210117112454225.png)

**寻找函数，将输入转化为输出**

三类：regression、classification、generation

- 回归

  回归方法是一种对数值型连续随机变量进行预测和建模的监督学习算法。

  方法：线性回归、回归树（随机森林、梯度提升树）、深度学习、

- 分类

  分类方法是一种对离散型随机变量建模或预测的监督学习算法。使用案例包括邮件过滤、金融欺诈和预测雇员异动等输出为类别的任务。

  方法：逻辑回归（通过逻辑函数将预测映射到0、1）、决策树CART、深度学习、支持向量机svm、朴素贝叶斯NB

- 聚类

  聚类是一种无监督学习任务，该算法基于数据的内部结构寻找观察样本的自然族群（即集群）。使用案例包括细分客户、新闻聚类、文章推荐等。

  因为聚类是一种无监督学习（即数据没有标注），并且通常使用数据可视化评价结果。如果存在「正确的回答」（即在训练集中存在预标注的集群），那么分类算法可能更加合适。

  方法：K均值聚类、Affinity Propagation 聚类

[上述三类来源](https://www.zhihu.com/search?type=content&q=%E5%9B%9E%E5%BD%92%E3%80%81%E5%88%86%E7%B1%BB%E3%80%81%E8%81%9A%E7%B1%BB)

方法：supervised learing 寻找loss最小的函数、reinforcement learning、unsupervised learing

## 回归

- 步骤
  - 定义一个model即function set
  - 定义一个goodness of function损失函数去评估该function的好坏
  - 找一个最好的function

## 深度学习

[详细笔记](https://sakura-gh.github.io/ML-notes/ML-notes-html/8_Deep-Learning.html)

### 步骤

- 定义一个神经网络**全连接层**（weight、bias、激励函数（非线性，增强模型表达能力））：

  - 神经元（图中小球）

    一个神经元是一个逻辑回归模型，一个神经元有x1、x2、...、xN个输入，每个输入xi对应一个wi即weight；每个神经元有一个b即bias；每个下式中关于z的函数叫sigmoid function（还可以是其它函数），在深度学习中叫激励函数
    $$
    f(x) = 1/(1 + e^{z}) = 1/(1 + e^{-(\sum ^{n} _{i=1} x_iw_i+b)})
    $$
  
  

  ![image-20210117163148148](C:\Users\GY\AppData\Roaming\Typora\typora-user-images\image-20210117163148148.png)

  ​															*图中每一排表示一个层，每个球表示一个神经元*

   输入层通过隐藏层，即一系列矩阵运算转化为输出层。若是分类问题则在通过在输出层后面加softmax，输出的y1...yM映射到0~1上变为z1....zM，每个z都在0~1上

- 定义一个神经网络的好坏 

  即output和target计算出loss，即y1...yM和我们给定的标签比较计算出loss

- 找到最好的神经网络，即找到最好的\theta（weight、bias）

  计算loss，求loss关于网络中参数的导数（链式法则求导），参数沿着导数方向变化

  梯度下降：正向传播算loss、计算梯度loss关于weight导数、反向传播更新weight
  
  工具：tensorflow、pytorch	

### tips

先与training set比较，看是否有好的结果，然后调参，再与testing set比较，看是否过拟合，然后调参。

- 关于激励函数

  sigmoid function：最原始的激励函数

  ReLU function：使激励函数变为类似线性的，从而避免梯度下降时变化不明显

  Maxout function： 可x成relu相同功能并且可以完成更多功能。因为input不同，不同max不同，则每个weight与bias都可以训练到

- 梯度下降时的learning rate

  adagrad

  RMSProp

- training set越来越好但testing set的loss可能上升

  **Early Stopping**：提前结束training，用validation set代替testing set，当validation set上升时即时停止

  **Regularization**：regularization就是在原来的loss function上额外增加几个term，比如我们要minimize的loss function原先应该是square error或cross entropy，那在做Regularization的时候，就在后面加一个Regularization的term

  **dropout**：training时，每次更新参数，对每个神经抽样，每个神经有一定几率被丢掉，若被丢掉则相连 的weight也被丢掉

### why deep

深层网络 ---》 [模组化](https://sakura-gh.github.io/ML-notes/ML-notes-html/14_Why-Deep.html) 

一层？多层？ 类似于逻辑电路，多层使逻辑门得到更多使用，更有效率。虽然一层即可达到任务，但是效率低下。

多层可以将输入进行多次映射，即可对复杂的问题进行分类。	

### CNN

影像识别

- 原因

  一个neural network也可识别。我们期待每个neuron是一个基本的分类器，然而input是图片时，输入的vector会特别长，而隐藏层的参数会更加多，我们希望简化网络的框架。即依靠先验知识对一些实际用不到的参数给过滤。

  - property1

    Some patterns are much smaller than the whole image

  - property2

    The same patterns appear in different regions

  - property3

    Subsampling the pixels will not change the object

    即去掉偶数行奇数列

- 步骤

  ​	convolution -》 maxpooling -》..... -》Flatten -》 Fully Connected Feedforward network

  - 卷积convolution

    - padding

      填充，即在卷积输入的图片周围填充一圈

      正常卷积尺寸变化： n * n    卷积   f * f  ----》 (n - f + 1)   * (n - f + 1)

      padding： (n + 2p) * (n + 2p)    卷积   f * f  ----》 (n + 2p - f + 1)   * (n  + 2p  - f + 1)  若p = (f-1)/2  则卷积后尺寸和卷积前相同

    - stride 步幅

      padding + stride： (n + 2p) * (n + 2p)    卷积   f * f   步幅s----》 (n + 2p - f)/s + 1   * (n  + 2p  - f)/s + 1  

    - 多通道卷积

      **假定用 N 表示filter的数量，那么每一个filter会生成一个 2 维的 feature(或matrix),N 个filter就生成 N 个 feature,N 个 feature 组成了卷积后的 featuremap，而 N 就是 featuremap 的通道数。**

      **filter的通道数要和卷积输入的通道数相同，若filter为立方体，那么卷积的结果也是立方体（方法就是每个通道分别卷积，结果叠起来就是立方体），在将该立方体各个通道对应坐标的值相加就生成了feature，相当于将多维的 Result 压缩成了 2 维的 feature（也可叫matrix，即矩阵）**

    - 意义

      用一打k * k的filter卷积图像中k*k的像素块得到的值，实质上就是该像素块作为神经网络输入，然后通过一层计算得到的输出，也就相当于过滤了一些用不到的参数。

      - filter为小范围：考虑到property1

      - 对图像用同一个filter卷积运算：考虑到property2

      用n个3 * 3 filter卷积1张6 * 6黑白图片 --> n个4 * 4的matrix（它们也叫做feature map 特征映射） 

    - 举例

      **【若为RGB图像，则一个filter为一个3 * 3 * 3立方体，卷积时每个3 * 3 * 3（长 * 宽 * 三通道）的filter和 4 * 4 * 3（长 * 宽 * 三通道）图像卷积，或者说看成3（通道数）个1  * （3 * 3）的filter 分别卷积 3（通道数）张 1 *（4 * 4）的图片，这时候每个卷积都为单通道（分别为R,G,B通道），卷积出3张再压缩为一张matrix 】**

      **【注意不论几次的convolution，每次convolution后都是得到filter个matrix】**  

      1. 25 * (3 * 3) filter 卷积  1 * (28 * 28) --> 25 * (26 * 26) feature map	

      2. 2 max pooling   --> 25 * (13 * 13)

      3. 50 * (3 * 3) filter 卷积 25 * (13 * 13)  --> 50 * (11 * 11) feature map   注意这里的filter不是一个3 * 3 的matrix，而是一个 25 * 3 * 3的立方体，因为原图像每个像素有25个通道，相应的feature map实际上也不是(11 * 11)的matrix而是 50 * 11 * 11

      4. 2 * 2 max pooling

      【filter也是经过不断训练得到的，训练时使参数不变filter变，梯度下降得到filter】
    
    - 卷积完输入激励函数

      对卷积完的特征图每一个元素都输入激励函数中进行运算	

  - 池化max pooling

    把得到的4 * 4matrix分为四个组，每个组用取平均值或者取最大值方法化为一个2 * 2的matrix

    同卷积，maxpooling取得区域相当于filter，也有步幅
  
  - Flatten + Fully..
  
    展开得到的matrix，然后放入全连接神经网络中得到输出

 相关资料：https://www.zhihu.com/question/49376084

多通道卷积：https://blog.csdn.net/briblue/article/details/83063170

### RNN

循环神经网络

 

### RESNET

- 原理

  增加shortcut，即例如layer1的输出直接连到layer2输出层的激励函数之前直接相加

- 意义

  解决梯度爆炸

# pytorch

- 使用

  文档：https://pytorch.org/docs/  查看详细内容

  pycharm  ctrl+左键  查看使用方法

- 基础知识

  pytoch tensor通道排序：[batch, channel, height, width]

  - tensor

    - tensor()取里面的值用.item()（注意得仅仅有一个值的时候）

    - tensor()的维度

      三维基于二维增加了一维，即（2，2，3）是包含了2个2行三列的矩阵；第一个数字即指包含几个二维矩阵

      四维基于三维增加了一维，（2，2，2，3）即包含了2个三维的矩阵。

      若是n维，以此类推，基于上一维增加一维计算。

    - tensor()索引操作

      ```python
          import torch as t  
            
          a = t.randn(3,4)  
          '''''tensor([[ 0.1986,  0.1809,  1.4662,  0.6693], 
                  [-0.8837, -0.0196, -1.0380,  0.2927], 
                  [-1.1032, -0.2637, -1.4972,  1.8135]])'''  
          print(a[0])         #第0行  
          '''''tensor([0.1986, 0.1809, 1.4662, 0.6693])'''  
          print(a[:,0])       #第0列  
          '''''tensor([ 0.1986, -0.8837, -1.1032])'''  
          print(a[0][2])      #第0行第2个元素，等价于a[0,2]  
          '''''tensor(1.4662)'''  
          print(a[0][-1])     #第0行最后一个元素  
          '''''tensor(0.6693)'''  
          print(a[:2,0:2])    #前两行，第0,1列  
          '''''tensor([[ 0.1986,  0.1809], 
                  [-0.8837, -0.0196]])'''  
            
          print(a[0:1,:2])    #第0行，前两列  
          '''''tensor([[0.1986, 0.1809]])'''  
          print(a[0,:2])      #注意两者的区别，形状不同  
          '''''tensor([0.1986, 0.1809])'''  
      ```

      即对应tensor维度取值，取出来还是一个tensor

- 构造网络模型

  `class  XXmodel(nn.Module):`继承模型

  `def __init__`中定义网络中的层次

  `nn.Conv2d`卷积层，ctrl+左键查看输入

  `nn.MaxPool2d`池化层

  `nn.Linear`全连接层

  `nn.Sequential`可以把卷积池化等层次打包进一个sequential中方便编写

  `def forward`中使用定义的层次完成正向传播

- 模型训练

  数据准备（含数据预处理，transform）：dataloader

  损失函数计算：lossfunction（也可直接写在模型搭建中）

  优化器定义：optimizer，需要传入model.parameter

  迭代训练+打印记录

# 图像分类

## LENET

- 网络构建

  ```python
  import torch
  import torch.nn as nn
  import torch.nn.functional as F
  
  class LeNet(nn.Module):
      def __init__(self):
          super(LeNet, self).__init__()
          self.conv1 = nn.Conv2d(3, 16, 5)
          self.pool1 = nn.MaxPool2d(2, 2)
          self.conv2 = nn.Conv2d(16, 32, 5)
          self.pool2 = nn.MaxPool2d(2, 2)
          self.fc1 = nn.Linear(32*5*5, 120)
          self.fc2 = nn.Linear(120, 84)
          self.fc3 = nn.Linear(84, 10)
  
      def forward(self, x):
          x = F.relu(self.conv1(x))    # input(3, 32, 32) output(16, 28, 28)
          x = self.pool1(x)            # output(16, 14, 14)
          x = F.relu(self.conv2(x))    # output(32, 10, 10)
          x = self.pool2(x)            # output(32, 5, 5)
          x = x.view(-1, 32*5*5)       # output(32*5*5)	
          x = F.relu(self.fc1(x))      # output(120)
          x = F.relu(self.fc2(x))      # output(84)
          x = self.fc3(x)              # output(10)
          return x
  
  
  ```

## ALEXNET

- 亮点

  1. gpu加速训练
  2. 使用relu激活函数
  3. 使用LRN局部响应归一化
  4. 全连接层前两层使用dropout随机失活神经元操作减少过拟合

- 详解

  <img src="C:\Users\GY\AppData\Roaming\Typora\typora-user-images\image-20210422162105333.png" alt="image-20210422162105333" style="zoom:80%;" />

- 模型搭建

  ```python
  import torch.nn as nn
  import torch
  
  
  class AlexNet(nn.Module):
      def __init__(self, num_classes=1000, init_weights=False):
          super(AlexNet, self).__init__()
          self.features = nn.Sequential(
              nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=2),  # input[3, 224, 224]  output[48, 55, 55]
              nn.ReLU(inplace=True),
              nn.MaxPool2d(kernel_size=3, stride=2),                  # output[48, 27, 27]
              nn.Conv2d(48, 128, kernel_size=5, padding=2),           # output[128, 27, 27]
              nn.ReLU(inplace=True),									# inplace使计算加速
              nn.MaxPool2d(kernel_size=3, stride=2),                  # output[128, 13, 13]
              nn.Conv2d(128, 192, kernel_size=3, padding=1),          # output[192, 13, 13]
              nn.ReLU(inplace=True),
              nn.Conv2d(192, 192, kernel_size=3, padding=1),          # output[192, 13, 13]
              nn.ReLU(inplace=True),
              nn.Conv2d(192, 128, kernel_size=3, padding=1),          # output[128, 13, 13]
              nn.ReLU(inplace=True),
              nn.MaxPool2d(kernel_size=3, stride=2),                  # output[128, 6, 6]
          )
          self.classifier = nn.Sequential(
              nn.Dropout(p=0.5),
              nn.Linear(128 * 6 * 6, 2048),
              nn.ReLU(inplace=True),
              nn.Dropout(p=0.5),
              nn.Linear(2048, 2048),
              nn.ReLU(inplace=True),
              nn.Linear(2048, num_classes),
          )
  
  
      def forward(self, x):
          x = self.features(x)
          x = torch.flatten(x, start_dim=1)
          x = self.classifier(x)
          return x
  ```

## VGG

- 亮点

  堆叠多个3*3卷积核来代替大尺度卷积核（减少所需参数）

  论文中提到通过堆叠两个3*3卷积核代替 5\*5的卷积核，堆叠三个3\*3卷积核代替7\*7卷积核
  $$
  F(i) = (F(i+1)-1)*Stride +Ksize	\\
  F(i)为第i层感受野

## GOOGLENET

- 亮点

  1. 引入inception结构（融合不同尺度信息）

     ![在这里插入图片描述](https://img-blog.csdnimg.cn/20190114172059481.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDEyMzEwOA==,size_16,color_FFFFFF,t_70)

     对并行得的每个特征图其宽和高必须相等，然后在深度上进行拼接，1*1卷积核是进行降维的操作

  2. 使用1*1卷积核进行降维以及映射处理

  3. 添加两个辅助分类器帮助训练

     - 5 * 5步长为3的平均池化层
     - 1 * 1卷积核降维到128深度
     - 1024全连接层
     - 70%的dropout层
     - 全连接层，连接softmax激活函数得到分类结果

  4. 丢弃全连接层，使用平均池化层（大大减少模型参数）

## RESNET

- 亮点

  1. 超深的网络结构
  2. 提出residual模块（残差结构）
  3. 使用batch normalization（BN）加速训练（丢弃dropout）




![image-20210513163309952](C:\Users\GY\AppData\Roaming\Typora\typora-user-images\image-20210513163309952.png)·

左残差结构

+：两个特征图在相同维度位置进行加法操作

relu：两个

右残差结构

注意两个参数对比式的第一个式子，它是用左残差结构但是64-d改为256-d得到的

![image_005](C:\Users\GY\AppData\Roaming\Typora\typora-user-images\image_005.jpg)

整体网络结构

其中｛｝中就对应左右两个残差结构



![image_003](C:\Users\GY\AppData\Roaming\Typora\typora-user-images\image_003.jpg)

每种残差结构还有两类

实线：主分支和shortcut分支维数相同可直接想加

虚线：主分支和shortcut分支维数不同，需要改变

上一张图中，每个coni_x中的｛｝会有虚线结构和实线结构



- batch  normallization

  在图像预处理时我们会将图像处理到归一化程度加速训练，然而卷积后的特征图并不满足这一归一化程度

  BN是为了调整输入的一批数据的每一层feature map总的分布，而不是某张图像

  ![image_004](C:\Users\GY\AppData\Roaming\Typora\typora-user-images\image_004.jpg)

  1.算均值和方差。一批数据的同一层的feature map同一个通道的所有数据求均值和方差

  2.归一化。减去均值除以方差，加上一个极小值防止分母为0

  3.进一步调整。/gama  /beta都是学习得到的

  - 注意问题

    训练时train参数为TRUE

    测试时为FALSE

    batchsize要大

    建议将BN层放在卷积和激活层的中间，且卷积不用bias（因为没有用）

- 迁移学习

  1.快速训练出一个理想结果

  2.数据集比较小也能训练出理想的效果

  原理：浅层网络的参数是比较通用的，参数可以迁移

  ![image](C:\Users\GY\AppData\Roaming\Typora\typora-user-images\image.jpg)



# 目标检测

## YOLO

- yolov1

  **grid cell**：图像划分成S×S的格子

  **bounding box**：**x,y,w,h,confidence**（Pr(Object) * IOU）

  **ground truth**：给定标注框

  **Pr(Object)**：如果ground truth落在这个grid cell里，那么Pr（Object）就取1，否则就是0

  **iou**：bounding box与实际的groud truth之间的交并比

  损失函数：bounding box损失 + confidence损失 + classes损失  

  > 作者在YOLO算法中把物体检测（object detection）问题处理成回归问题，用一个卷积神经网络结构就可以从输入图像直接预测bounding box和类别概率。
  >
  > 算法首先把输入图像划分成S×S的格子**grid cell**，然后对每个格子都预测B个**bounding boxes**，每个bounding box都包含5个预测值：**x,y,w,h**和**confidence**。
  > x，y就是bounding box的中心坐标，与grid cell对齐（即相对于当前grid cell的偏移值），使得范围变成0到1；
  > w，h进行归一化（分别除以图像的w和h，这样最后的w和h就在0到1范围）。
  > confidence代表了所预测的box中含有object的置信度和这个box预测的有多准两重信息：
  > 换句话说，如果ground truth落在这个grid cell里，那么Pr（Object）就取1，否则就是0，**IOU**就是bounding box与实际的groud truth之间的交并比。所以confidence就是这两者的乘积。
  >
  > 在yolov1中作者将一幅图片分成7x7个网格(grid cell)，由网络的最后一层输出7×7×30的tensor，也就是说每个格子输出1×1×30的tensor。30里面包括了2个bound ing box的x，y，w，h，confidengce以及针对格子而言的20个类别概率，输出就是 7x7x(5x2 + 20) 。
  > (通用公式： SxS个网格，每个网格要预测B个bounding box还要预测C个categories，输出就是S x S x (5×B+C)的一个tensor。 注意：class信息是针对每个网格的，confidence信息是针对每个bounding box的
  >
  > 

- yolov2

  1. BN

  2. High Resolution Classifier  高分辨率

     输入图片分辨率更大

  3. Convolutional With Anchor Boxes  使用先验框

  4. *Dimension* *Clusters*(使用聚类算法提取anchor boxes的宽高)

  5. Direct location prediction

  6. Fine-Grained Features（使用passthrough layer 融合大小不同的特征图）

  7. Multi-Scale Training  多尺度训练

     没迭代10个batch，就将网络输入尺寸进行随机的选择（为32的整数倍，32为缩放因子）

  骨干网络：darknet

- yolov3

  [从零开始学习yolov3](https://www.cnblogs.com/pprp/p/12199731.html)

  骨干网络：darknet-53	

  - 模型结构

    有3个预测特征层，每个预测特征层的每个cell有3个bounding box，所以每个预测特征层维度为 N * N * (3 * (4+1+80))  以coco为例（3：3个box，4：box的坐标偏移信息，1：confidence，80：80个类），3（预测特征层） * 3（3个bounding box） 就为anchor个数

  - 目标边界框的预测

$$
$$b_x = \sigma(t_x) + c_x \\
b_y = \sigma(t_y) + c_y \\
b_w = p_w e ^{t_w}	\\
b_h = p_h e ^{t_h}	\\
c_x \  c_y为cell左上角坐标 \\
t_x \ t_y为anchor偏移	\\
\sigma为sigmod函数，防止anchor偏移到别的cell
每个cell有三个box，box
$$



  - 损失计算

    我们的预测值形象化为每个cell都有9个bounding box，而数值化即为N * N * (3 * (4+1+80))，即用这个向量来计算损失

    - 正负样本选择

      **正样本的选择：**
      首先计算目标中心点落在哪个cell上，然后计算这个cell的9个先验框（anchor）和目标真实位置的IOU值（直接计算，不考虑二者的中心位置），取IOU值最大的先验框和目标匹配。于是，找到的 该grid中的 该anchor 负责预测这个目标，其余的网格、anchor都不负责。

      **负样本的选择：**
       计算各个先验框和所有的目标ground truth之间的IOU，如果某先验框和图像中所有物体最大的IOU都小于阈值（一般0.5），那么就认为该先验框不含目标，记作负样本，其置信度应当为0

      **不参与计算部分**
       这部分虽然不负责预测对象，但IOU较大，可以认为包含了目标的一部分，不可简单当作负样本，所以这部分不参与误差计算。

    - loss计算

      正样本：置信损失+分类损失+定位损失

      负样本：置信损失

      不参与计算：无

- yolov3-spp

  - mosaic图像增强

    图像旋转拼接...，增加数据多样性增加目标个数

  - spp结构

    网络结构加入spp结构融合不同尺度特征

  - 损失函数

    引入IoU loss

    引入GIoU Loss

    引入DIoU Loss，CIoU Loss

  - focal loss

    关于one-stage模型（ssd，yolo），存在问题：正负样本不平衡，没匹配到的候选框远大于匹配到的

## FAST-RCNN

- rcnn
  - 步骤
    1. 一张图像生成1k~2K个候选区
    
       假定2000个
    
    2. 对每个候选区，使用深度网络提取特征
    
       候选区缩放到227*227，输入分类网络（无全连接层）得到2000 * 4096维矩阵
    
    3. 特征送入每一类的SVM分类器，判别是否属于该类
    
       2000 * 4096 与 4096 * 20（20个类别）的svm分类器相乘，得到2000 * 20 维矩阵，表示每个候选区的每个目标类别的得分，得分最高的就为该类建议框。对每个类所有建议框使用**非极大值抑制**剔除重叠建议框
    
    4. 使用回归器精修候选框位置
  
- fast-rcnn

  1. 一张图像生成1k~2K个候选区

  2. 将整幅图像输入网络得到相应的特征图，将SS算法生成的候选框投影到特征图上获得相应的特征矩阵

     ---》候选框的特征不需要重复计算

  3. 将每个特征矩阵通过ROI pooling层缩放到 7x7 大小的特征图，接着将特征图展平通过一系列全连接层得到预测结果

     - 训练数据采样（正负样本）

       候选框和gt交并比IoU大于0.5为正样本，对应类别概率为gt相应类别[0,1,0,0,....]；

       候选框和gt交并比IoU小于一定阈值则为负样本，对应N+1类别概率就为[1,0,0,....]，即标记为背景，即可计算分类损失；边界框回归损失就为0

     - 7 * 7 特征图并连出两个分支，通过一系列全连接层：一个输出 N+1个类别的概率（+1因为N+1维向量中有个背景概率）；还有一个输出(N+1)*4个节点，因为每个候选框回归参数为(dx,dy,dw,dh)

     - 损失计算

       分类损失+边界框回归损失

       分类损失：正负样本的类别概率和与其实际类别计算

       边界框回归损失：负样本为0；正样本为gt的边界框和预测的候选框进行计算

       训练分类器内的参数和候选框回归参数

- faster-rcnn

  1. 将图像输入网络得到相应的特征图A
  2. 使用RPN结构生成候选框，将RPN生成的候选框投影到特征图上获得相应的特征矩阵
  3. 将每个特征矩阵通过ROI pooling层缩放到 7x7 大小的特征图，接着将特征图展平通过一系列全连接层得到预测结果

  因此，相较于fast-rcnn区别就是使用rpn获得候选框

  - RPN

    输入：N * N * depth大小特征图

    输出：N * N * 2k score和N * N * 4k reg

    - 实现

      原图输入到ZF或VGG16网络中得到特征图，其深度为256或512，（感受野大小：ZF网络中，特征图中3 * 3 对应原图171 * 171，依据卷积尺寸变化计算）

      用3 * 3卷积核（深度256，个数256，stride =1，padding = 1）卷积特征图，相当于用3 * 3滑动窗口检测，**每个**3 * 3滑动窗口都可以得到一个1 * 1 * 256的向量，该向量输入到cls layer得到2k维 score，同时输入到reg layer得到4k维anchor大小信息。【输入到cls layer和reg layer的过程实际上是用3 * 3卷积后，再用2k个1 * 1和4k * 1 * 1卷积核分别卷积得到结果】

      k：anchor个数，faster-rcnn中为 三种面积{128，256，512} 三种比例{1:1, 1:2, 2:1}总计k=9

      2k score：score表示每个anchor其为背景和为物体的概率，因此2 * k

      4k reg：每个anchor需要{dx,dy,dw,dh}确定大小和位置

      注意anchor定位要定位到原图而不是特征图

    - 损失计算

      - 训练数据采样（正负样本）

        每个batch，随机选取256个anchor且正负比为1：1，正样本少于128则用负样本填充

        正样本：和gt的IoU超过0.7或者和某个gt拥有最大的IoU

        负样本：与所有gt的IoU都小于0.3

      - 损失

        分类损失+边界框回归损失

        正负样本的 score和reg与 gt对比计算得出

## SSD

# 图像分割

本质：是像素级的分类，对于图中的每一个像素给予一个像素级的label,即对于每一个像素归属到一个类别。

- 语义分割

  语义分割是对图像中的每个像素打上类别标签，把图像分为车（紫色）、斑马线（黄色）、人（绿色）、天空（灰色）、建筑（红色），对于同一物体的不同实例不需要单独分割。

- 实例分割

  目标检测和语义分割的结合。相对于目标检测的边界框，实力分割可以精确到物体的边缘；相对于语义分割，实例分割需要区分同一个物体的不同个体

- 全景分割

  全景分割是语义分割和实例分割的结合。与实例分割不同，其只对图像中的物体进行检测和分割，而全景分割是对图像中的所有物体包括背景都要进行检测和分割。

- 三种分割类型的定义： 

  语义分割：给每个pixel分类

  实例分割：给每个检测框的里的object分mask

  全景分割：背景pixel分类+框里mask

- 评价指标

  mIOU：每一个类比交并比的平均值

  mAcc：Pred和GT对应位置的“分类”准确率（分对的像素 / 所有像素）

## 基于深度学习的图像分割

不会就查文档、翻ai studio项目案例、
写网络结构时查看model.summary查看模型整体框架对照代码理解

- 基本流程

  **输入**：图像3 * H * W（RGB）+标注图像（标注方法：每个像素0，1，2，3表示不同类别）

  **算法**：深度学习模型

  **输出**：特征图 C * H * W（C维表示属于每个类别的概率），再通过取概率最大值得到 1 * H * W的分割图像

- 语义分割

  为了增强深度卷积网络的表现力，通常需要在网络更深时增加特征图（channels）的数量。 

  - 主流方法

    一种主流方法是遵循编码器/解码器（encoder/decoder）结构: 

    先对输入进行下采样，得到较低分辨率的特征映射，其学习到了如何高效地区分各个类。 

    然后对这些特征进行上采样以得到一个原始分辨率的分割结果。
    
    - 上采样方法
      1. 双线性插值
      2. unpooling
      3. 转置卷积
    
  - 全卷积分割网络FCN

    在VGG-16的基础上将全连接层替换为卷积层，**输出空间映射**而不是分类分数。这些映射由小步幅卷

    积上采样（又称反卷积）得到，来产生密集的像素级别的预测。

    - 问题一

      编码模块将输入分辨率降低32倍，解码模块很难得到细粒度的分割

      解决方法：添加跳跃连接。对编码的特征缓慢上采样（分阶段）来解决上面的问题，并添加了从前面层的跳

      跃连接，最后对这样的两个特征图求和。

  - U-Net

    ![img](https://pic2.zhimg.com/80/v2-68ffbaff593f95cc96fc4b6811356e39_hd.jpg)

    上采样（绿色箭头） +  下采样（蓝色箭头） + 跳跃连接（灰色箭头）

    编码器：遵循典型的卷积网络结构，由两个重复的3\*3卷积核组成，且均使ReLU 激活函数和一个用于下采样的步长为2 的2*2最大池化操作，在每一个下采样的步骤中，特征通道数量都加倍。

    解码器：每一步都包含对特征图进行上采样，然后用2\*2的卷积核进行卷积运算，用于减少一半的特征通道数量，接着级联编码器中裁剪后的特征图，再用两个3\*3的卷积核进行卷积运算，均使用ReLU激活函数。由于在每次卷积操作中，边界像素存在缺失问题，因此有必要对特征图进行裁剪。 在最后一层，用1\*1的卷积核进行卷积运算，将每个64维的特征向量映射到输出层。

    下采样：

    2 \* 2 池化层  ---》 3*3 padding1卷积核 ---》 BN层  ---》 relu函数  ---》 3\*3 padding1卷积核  ---》 BN层  ---》relu函数

    尺寸大约/2  通道数*2	

    ```python
    class DoubleConv(paddle.nn.Layer):
        """(convolution => [BN] => ReLU) * 2"""
        def __init__(self, in_channels, out_channels):
            super(DoubleConv, self).__init__()
    
            self.double_conv = paddle.nn.Sequential(
                paddle.nn.Conv2D(in_channels, out_channels, kernel_size=3, padding=1),
                paddle.nn.BatchNorm2D(out_channels),
                paddle.nn.ReLU(),
                paddle.nn.Conv2D(out_channels, out_channels, kernel_size=3, padding=1),
                paddle.nn.BatchNorm2D(out_channels),
                paddle.nn.ReLU()
            )
     
        def forward(self, x):
            return self.double_conv(x)
        
    class Down(paddle.nn.Layer):
        """Downscaling with maxpool then double conv"""
        def __init__(self, in_channels, out_channels):
            super(Down, self).__init__()
            self.maxpool_conv = paddle.nn.Sequential(
                paddle.nn.MaxPool2D(kernel_size=2, stride=2, padding=0),
                DoubleConv(in_channels, out_channels)
            )
     
        def forward(self, x):
            return self.maxpool_conv(x)
    ```
    
    上采样：
    
    x1双线性上采样层 ---》  x1补足到x2大小   ---》 x1 x2concat操作 ---》 双卷积  	
    
    ```python
    class Up(paddle.nn.Layer):
        """Upscaling then double conv"""#实现上采样和跳跃连接，飞浆里边封装好的，双线性。
     
        def __init__(self, in_channels, out_channels, bilinear=True):
            super(Up, self).__init__()
     
            if bilinear:
                self.up = paddle.nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
            else:
                self.up = paddle.nn.ConvTranspose2d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)
     
            self.conv = DoubleConv(in_channels, out_channels)
     
        def forward(self, x1, x2):
            x1 = self.up(x1)
            # print(x2.shape, x1.shape)
            # print(x2.shape[2] - x1.shape[2])
            diffY = paddle.to_tensor([x2.shape[2] - x1.shape[2]])#对比长宽，防止尺寸不一样
            diffX = paddle.to_tensor([x2.shape[3] - x1.shape[3]])
     
            x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])
     
            x = paddle.concat([x2, x1], axis=1)
            return self.conv(x)
    ```
    
    模型整体架构：
    
    ```python
    class U_Net(paddle.nn.Layer):
        def __init__(self, num_classes, bilinear=True):
            super(U_Net, self).__init__()
            self.num_classes=num_classes
            self.bilinear=bilinear
            self.inc=DoubleConv(3,64)
            self.down1=Down(64,128)
            self.down2=Down(128,256)
            self.down3=Down(256,512)
            self.down4=Down(512,512)
            self.up1=Up(1024,256,bilinear)   # 1024是因为下采样最后一层down4输出层（通道为512），还要和down3输出的层（通道512）拼接，因此输入通道为1024；
            self.up2=Up(512,128,bilinear)
            self.up3=Up(256,64,bilinear)
            self.up4=Up(128,64,bilinear)
    
            self.output_conv=paddle.nn.Conv2D(64,num_classes,kernel_size=1)
        
        def forward(self, inputs):
            x1=self.inc(inputs)
            x2=self.down1(x1)
            x3=self.down2(x2)
            x4=self.down3(x3)
            x5=self.down4(x4)
            x=self.up1(x5,x4)
            x=self.up2(x,x3)
            x=self.up3(x,x2)
            x=self.up4(x,x1)
    
    
            y=self.output_conv(x)
    
            return y
    -------------------------------------------------------------------------------------------
     Layer (type)               Input Shape                  Output Shape         Param #    
    ===========================================================================================
       Conv2D-1              [[1, 3, 160, 160]]           [1, 64, 160, 160]        1,792     
     BatchNorm2D-1          [[1, 64, 160, 160]]           [1, 64, 160, 160]         256      
        ReLU-1              [[1, 64, 160, 160]]           [1, 64, 160, 160]          0       
       Conv2D-2             [[1, 64, 160, 160]]           [1, 64, 160, 160]       36,928     
     BatchNorm2D-2          [[1, 64, 160, 160]]           [1, 64, 160, 160]         256      
        ReLU-2              [[1, 64, 160, 160]]           [1, 64, 160, 160]          0       
     DoubleConv-1            [[1, 3, 160, 160]]           [1, 64, 160, 160]          0       
      MaxPool2D-1           [[1, 64, 160, 160]]            [1, 64, 80, 80]           0       
       Conv2D-3              [[1, 64, 80, 80]]             [1, 128, 80, 80]       73,856     
     BatchNorm2D-3           [[1, 128, 80, 80]]            [1, 128, 80, 80]         512      
        ReLU-3               [[1, 128, 80, 80]]            [1, 128, 80, 80]          0       
       Conv2D-4              [[1, 128, 80, 80]]            [1, 128, 80, 80]       147,584    
     BatchNorm2D-4           [[1, 128, 80, 80]]            [1, 128, 80, 80]         512      
        ReLU-4               [[1, 128, 80, 80]]            [1, 128, 80, 80]          0       
     DoubleConv-2            [[1, 64, 80, 80]]             [1, 128, 80, 80]          0       
        Down-1              [[1, 64, 160, 160]]            [1, 128, 80, 80]          0       
      MaxPool2D-2            [[1, 128, 80, 80]]            [1, 128, 40, 40]          0       
       Conv2D-5              [[1, 128, 40, 40]]            [1, 256, 40, 40]       295,168    
     BatchNorm2D-5           [[1, 256, 40, 40]]            [1, 256, 40, 40]        1,024     
        ReLU-5               [[1, 256, 40, 40]]            [1, 256, 40, 40]          0       
       Conv2D-6              [[1, 256, 40, 40]]            [1, 256, 40, 40]       590,080    
     BatchNorm2D-6           [[1, 256, 40, 40]]            [1, 256, 40, 40]        1,024     
        ReLU-6               [[1, 256, 40, 40]]            [1, 256, 40, 40]          0       
     DoubleConv-3            [[1, 128, 40, 40]]            [1, 256, 40, 40]          0       
        Down-2               [[1, 128, 80, 80]]            [1, 256, 40, 40]          0       
      MaxPool2D-3            [[1, 256, 40, 40]]            [1, 256, 20, 20]          0       
       Conv2D-7              [[1, 256, 20, 20]]            [1, 512, 20, 20]      1,180,160   
     BatchNorm2D-7           [[1, 512, 20, 20]]            [1, 512, 20, 20]        2,048     
        ReLU-7               [[1, 512, 20, 20]]            [1, 512, 20, 20]          0       
       Conv2D-8              [[1, 512, 20, 20]]            [1, 512, 20, 20]      2,359,808   
     BatchNorm2D-8           [[1, 512, 20, 20]]            [1, 512, 20, 20]        2,048     
        ReLU-8               [[1, 512, 20, 20]]            [1, 512, 20, 20]          0       
     DoubleConv-4            [[1, 256, 20, 20]]            [1, 512, 20, 20]          0       
        Down-3               [[1, 256, 40, 40]]            [1, 512, 20, 20]          0       
      MaxPool2D-4            [[1, 512, 20, 20]]            [1, 512, 10, 10]          0       
       Conv2D-9              [[1, 512, 10, 10]]            [1, 512, 10, 10]      2,359,808   
     BatchNorm2D-9           [[1, 512, 10, 10]]            [1, 512, 10, 10]        2,048     
        ReLU-9               [[1, 512, 10, 10]]            [1, 512, 10, 10]          0       
       Conv2D-10             [[1, 512, 10, 10]]            [1, 512, 10, 10]      2,359,808   
    BatchNorm2D-10           [[1, 512, 10, 10]]            [1, 512, 10, 10]        2,048     
        ReLU-10              [[1, 512, 10, 10]]            [1, 512, 10, 10]          0       
     DoubleConv-5            [[1, 512, 10, 10]]            [1, 512, 10, 10]          0       
        Down-4               [[1, 512, 20, 20]]            [1, 512, 10, 10]          0       
      Upsample-1             [[1, 512, 10, 10]]            [1, 512, 20, 20]          0       
       Conv2D-11            [[1, 1024, 20, 20]]            [1, 256, 20, 20]      2,359,552   
    BatchNorm2D-11           [[1, 256, 20, 20]]            [1, 256, 20, 20]        1,024     
        ReLU-11              [[1, 256, 20, 20]]            [1, 256, 20, 20]          0       
       Conv2D-12             [[1, 256, 20, 20]]            [1, 256, 20, 20]       590,080    
    BatchNorm2D-12           [[1, 256, 20, 20]]            [1, 256, 20, 20]        1,024     
        ReLU-12              [[1, 256, 20, 20]]            [1, 256, 20, 20]          0       
     DoubleConv-6           [[1, 1024, 20, 20]]            [1, 256, 20, 20]          0       
         Up-1       [[1, 512, 10, 10], [1, 512, 20, 20]]   [1, 256, 20, 20]          0       
      Upsample-2             [[1, 256, 20, 20]]            [1, 256, 40, 40]          0       
       Conv2D-13             [[1, 512, 40, 40]]            [1, 128, 40, 40]       589,952    
    BatchNorm2D-13           [[1, 128, 40, 40]]            [1, 128, 40, 40]         512      
        ReLU-13              [[1, 128, 40, 40]]            [1, 128, 40, 40]          0       
       Conv2D-14             [[1, 128, 40, 40]]            [1, 128, 40, 40]       147,584    
    BatchNorm2D-14           [[1, 128, 40, 40]]            [1, 128, 40, 40]         512      
        ReLU-14              [[1, 128, 40, 40]]            [1, 128, 40, 40]          0       
     DoubleConv-7            [[1, 512, 40, 40]]            [1, 128, 40, 40]          0       
         Up-2       [[1, 256, 20, 20], [1, 256, 40, 40]]   [1, 128, 40, 40]          0       
      Upsample-3             [[1, 128, 40, 40]]            [1, 128, 80, 80]          0       
       Conv2D-15             [[1, 256, 80, 80]]            [1, 64, 80, 80]        147,520    
    BatchNorm2D-15           [[1, 64, 80, 80]]             [1, 64, 80, 80]          256      
        ReLU-15              [[1, 64, 80, 80]]             [1, 64, 80, 80]           0       
       Conv2D-16             [[1, 64, 80, 80]]             [1, 64, 80, 80]        36,928     
    BatchNorm2D-16           [[1, 64, 80, 80]]             [1, 64, 80, 80]          256      
        ReLU-16              [[1, 64, 80, 80]]             [1, 64, 80, 80]           0       
     DoubleConv-8            [[1, 256, 80, 80]]            [1, 64, 80, 80]           0       
         Up-3       [[1, 128, 40, 40], [1, 128, 80, 80]]   [1, 64, 80, 80]           0       
      Upsample-4             [[1, 64, 80, 80]]            [1, 64, 160, 160]          0       
       Conv2D-17            [[1, 128, 160, 160]]          [1, 64, 160, 160]       73,792     
    BatchNorm2D-17          [[1, 64, 160, 160]]           [1, 64, 160, 160]         256      
        ReLU-17             [[1, 64, 160, 160]]           [1, 64, 160, 160]          0       
       Conv2D-18            [[1, 64, 160, 160]]           [1, 64, 160, 160]       36,928     
    BatchNorm2D-18          [[1, 64, 160, 160]]           [1, 64, 160, 160]         256      
        ReLU-18             [[1, 64, 160, 160]]           [1, 64, 160, 160]          0       
     DoubleConv-9           [[1, 128, 160, 160]]          [1, 64, 160, 160]          0       
         Up-4       [[1, 64, 80, 80], [1, 64, 160, 160]]  [1, 64, 160, 160]          0       
       Conv2D-19            [[1, 64, 160, 160]]            [1, 4, 160, 160] 
    ```
    
    
    
    - 变体
    
      1. 扩展ResNet
    
         ![TB1bwmaL7voK1RjSZFwXXciCFXa.jpg](https://gw.alicdn.com/tfscom/tuitui/TB1bwmaL7voK1RjSZFwXXciCFXa.jpg)
    
         （a）具有长跳跃连接 的ResNet，（b）Bottleneck块，（c）Basic块，（d）Simple块。 （蓝色：可选下采样，黄色：可选上采样） 
    
         在块内引入短路连接，使得训练、收敛更快
    
      2. FC-DenseNets
    
      3. Unet++

- 实例分割

  - mask-rcnn

    fast-rcnn有两个分支：分类、检测

    mask-rcnn增加一个独立的分割分支，即三个分支：分类、检测、分割，分割最终输出为 m * m * K，K为物体类别数，m * m为输出特征图长宽。因此损失计算也变为三个部分，Lcls + Lbox + Lmask，最终结果就是输出候选框，候选框内完成分割，合起来就完成 了实例分割

    同时增加一个ROI aligning用于对齐，由于从候选区（原图框定出的区域）提取特征图（假设为7 * 7大小），即原图长宽/7，对该7 * 7 = 49 个的每个格子BOX_49，再平均分为4个矩形BOX_4，每个矩形BOX_4中心点内通过双线性插值求值，对这4个值再取最大值就得到BOX_49的值，也就得到了特征图的值。**这样特征图映射回原图就不会产生偏差**

    1. 将faster-rcnn的ROI POOLING改为ROI aligning增加精度
    2. 新增一个分支FCN用于对ROI aligning后的特征图进行分割

  - YOLACT: Real-time Instance Segmentation

    就是快，快很多，但精确率一般

  

# 视频分类

- 研究问题

  将一段视频分类到预先指定类别集合中的某一个或多个

## RNN

![img](https://pic4.zhimg.com/80/v2-3884f344d71e92d70ec3c44d2795141f_720w.jpg)

X：输入层的值（向量）

S：隐藏层的值（向量）

O：输出层的值（向量）

U：输入层到隐藏层的权重矩阵

V：隐藏层到输出层的权重矩阵

w：上一个隐藏层输入到下一个隐藏层的权重矩阵

- **公式计算**

$$
O_t = g(V \cdot S_t) \\
S_t = f(U \cdot X_t + W \cdot S_{t-1})
$$

- loss

  loss根据不同任务具体来定义的，如下

  1. RNN用作表示句子的含义，loss为最后RNN输出的结果和真实标签的loss

     ![img](https://pic2.zhimg.com/80/v2-a87f64b550a1be099d409d91947d7ce5_720w.jpg)

  2. RNN表示上下文语境的应用，loss为所有时刻loss之和

     ![img](https://pic3.zhimg.com/80/v2-a40a0a5308d89ad46868fd9a69322ba6_720w.jpg)

## LSTM

![img](https://pic4.zhimg.com/v2-561a4eeb23e5bdd3a0bb08d818511aa3_b.jpg)

h_t-1：t-1时刻的隐藏层

x_t：t时刻的输入向量

h_t：（加softmax即可作为真正输出，否则作为隐藏层）

c_t-1、c_t：主线/记忆

橙色部分：h_t-1和 x_t联合起来控制了三个门，并且是输入的唯一来源，所以划分为输入部分。

h_t向上与向右：向上隔了一层softmax（图中没画），是输出的直接来源；向右是下一个LSTM cell的输入

- 公式

  - 遗忘门

    ![[公式]](https://www.zhihu.com/equation?tex=f_t%3D%5Csigma%28U_fh_%7Bt-1%7D%2BW_fx_t%29+%5Ctag%7B%E9%81%97%E5%BF%98%E9%97%A8%7D)

    ![[公式]](https://www.zhihu.com/equation?tex=k_t%3Dc_%7Bt-1%7D%5Codot+f_t+%5Ctag%7B%E4%B8%BB%E7%BA%BF%E9%81%97%E5%BF%98%7D)

  - 输入门

    ![[公式]](https://www.zhihu.com/equation?tex=i_t%3D%5Csigma%28U_ih_%7Bt-1%7D%2BW_ix_t%29+%5Ctag%7B%E8%BE%93%E5%85%A5%E9%97%A8%7D)

    ![[公式]](https://www.zhihu.com/equation?tex=g_t%3Dtanh%28U_gh_%7Bt-1%7D%2BW_gx_t%29+%5Ctag%7B%E8%A1%A5%E7%BB%99%E6%9D%A5%E6%BA%90%7D)

    ![[公式]](https://www.zhihu.com/equation?tex=j_t%3Dg_t%5Codot+i_t+%5Ctag%7B%E9%97%A8%E6%8E%A7%E5%88%B6%E8%A1%A5%E7%BB%99%E5%A4%A7%E5%B0%8F%7D)

    ![[公式]](https://www.zhihu.com/equation?tex=c_t%3Dj_t%2Bk_t+%5Ctag%7B%E8%A1%A5%E7%BB%99%E4%B8%BB%E7%BA%BF%7D)

  - 输出门

    ![[公式]](https://www.zhihu.com/equation?tex=o_t%3D%5Csigma%28U_oh_%7Bt-1%7D%2BW_ox_t%29+%5Ctag%7B%E8%BE%93%E5%87%BA%E9%97%A8%7D)

    ![[公式]](https://www.zhihu.com/equation?tex=h_t%3Dtanh%28c_t%29%5Codot+o_t+%5Ctag%7B%E4%B8%BB%E7%BA%BF%E7%94%9F%E6%88%90%E8%BE%93%E5%87%BA%7D)

  其中U、W即为权重

# video transform

# GAN

无监督
