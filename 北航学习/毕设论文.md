## 课题背景及目的

​		痤疮是毛囊皮脂腺单位的一种多因素疾病。临床表现从轻度的粉刺型痤疮到暴发型伴有系统性症状的痤疮，主要表现形态有粉刺、丘疹、脓疱、结节、囊肿，等等。痤疮通常被认为是青少年的一种疾病，越85%的12-24岁的青少年患此病。尽管寻常痤疮大多见于青少年人群，但是有大约3%的男性和12%的女性会患病持续到44岁[1]。痤疮往往会改变患者的容貌外观，这种改变经常会造成患者的自卑情绪甚至上升到社会隔离，而不规范的治疗手段往往会加重患者面部痤疮严重程度，更加会对患者的身心健康造成严重并且不可逆的伤害。同时，痤疮在经济上的影响也是无可否认的。例如在美国，每年会花费超过25亿美元在治疗痤疮上，这也是对医疗资源的沉重负担。因此，及时规范的痤疮治疗是十分有必要的。

​		传统的痤疮治疗，需要先由皮肤科医生在临床环境中对患者的痤疮严重程度进行评估，再由医生进行处方治疗或根据严重程度推荐非处方护肤品。在这之中，对痤疮严重程度准确评级是及时规范的痤疮治疗的基础。痤疮严重程度的分级标准是不完全统一的，其中包括国际改良分类法、GAGS分类法、Pillsbury分类法、MDI分类法、Hayashi分类法[2]，这些分类法依靠的主要是简单计数皮损个数以及皮损的种类来划分痤疮的严重程度，例如Hayashi分类法将炎性皮损数少于6个分为轻度，炎性皮损数在6-20个分为中度，炎性皮损数在21-50个分为重度，炎性皮损数大于60个分为极重度。因此皮损个数的精确计数在对痤疮严重程度评级上起了决定性的作用。

​		VISIA皮肤检测仪[3]是一种能对皮肤的病理学特征进行定量分析的仪器。它是皮肤检测仪器的一个重要里程碑，使用超高清的摄像头，并通过白光、紫外光、偏振光三种光源扫描皮肤表层和深层，从而检测出斑点以及毛孔、皱纹、皮肤纹理、皮肤细菌状态等信息。依靠VISA皮肤检测仪中的白光拍摄，可以得到患者面部痤疮的详细信息，清晰地展现痤疮的位置、种类、分布等信息，对开展皮肤痤疮研究工作有着重要意义。

​		本项目旨在使用深度学习方法，搭建自动检测人脸痤疮位置以及数目的模型，并使用VISIA皮肤检测仪收集到的数据来训练和评估模型。本项目构建的模型可以给患者以及低年资的医生提供客观的、可靠的、快速的评估意见，帮助其进行痤疮分级评估，从而采取更加科学准确的治疗方法，减少不规范的治疗，这对患者以及整个社会都有重大的意义。

## 国内外研究现状

### 传统机器学习算法

在图像视觉领域中，传统的机器学习算法模型主要分为5个步骤，分别为特征感知、图像预处理、特征提取、特征筛选、推理预测与识别。而对于传统目标检测任务，它通常需要选择检测窗口、提取图像特征、设计分类器这三个步骤，并且往往所提取的特征主要围绕底层特征和中层次特征来展开，例如颜色、纹理等。曾经出现过许多优秀的检测算法，例如有Viola-Jones人脸检测器主要用于人脸的检测，有向梯度直方图(Histogram of Oriented Gradients，HOG)[4]与支持向量机（Support Vector Machine，SVM）[5]算法的出现被应用于行人检测，以及可变型部件模型算法（Deformable Part Model，DPM）[6]在深度学习方法成熟之前一直在目标检测中发挥作用。

传统的机器学习方法也有被用于皮肤痤疮的检测。于2016年Nasim Alamdari提出了一种面部皮肤缺陷自动检测与识别系统[7]，该系统提出了几种图像分割方法来检测痤疮病灶位置和几种机器学习方法来区分不同的痤疮病灶的类别。论文使用了纹理分析、k均值聚类、HSV模型分割技术来检测痤疮病灶位置。其中k均值聚类法优于其它几个方法，检测准确率达到大约70%。而在痤疮病灶的分类问题中，论文使用了模糊C均值聚类算法和支持向量机方法，在鉴别痤疮疤痕和活动性炎症性病变问题中分别取得80%和66.6%的准确率，同时模糊C均值聚类算法在鉴别正常皮肤和病变皮肤的问题上准确率达到了100%。然而，本文提出的方法依赖于手工设计的特征来完成痤疮的检测与分类任务。同时，本文使用了有限数量的图像来评估分割和分类任务，评估方法仅仅采用35幅图像来检测分割和分类精度，这是无法对该论文提出的方法做到全面评估的，同时也不能保证方法的鲁棒性，因此需要获得更多的图像来检测该系统检测不同病灶的实用性。另外，所需要的图像需要有特定的规定，所有图像大小相同(512 * 512)、8位灰度分辨率和24位每像素的分辨率，如果图像有其它结构的存在（例如头发或）则会大大降低检测的准确性，整个系统的鲁棒性存在着比较大的问题。

尽管传统的机器学习方法在某些具体任务中有不错的表现，然而针对每个具体的任务，由于传统方法依赖于手工设计的特征，都需要研究人员有丰富的经验，才能设计出合适的特征算法，此外还需要合适的分类器，才能取得预期的效果，而这些条件相对来说是非常苛刻的，因此也很难达到最优的效果。

### 深度学习算法

近些年来，深度学习（Deep Learning）理论被广泛应用于图像视觉领域，而其中卷积神经网络是应用最广泛的一种神经网络结构，它在1986年由LeCun提出。在早期卷积神经网络被成功用于手写字符图像识别，而在2012年AlexNet这个更深层次的神经网络结构取得成功后，卷积神经网络蓬勃发展，被广泛用于各个领域。而在视觉上，包括图像分类、图像检测、图像分割等问题卷积神经网络都有所应用。

而医学疾病的诊断也越来越受到图像视觉研究领域人员的关注。在许多计算机视觉任务上取得良好性能的深度学习方法也被用于医学影像领域。在2017年斯坦福大学在Nature杂志上发表了一篇研究，它们使用卷积神经网络来鉴别恶性黑色素瘤与良性色素痣、角质形成细胞癌与良性脂溢性角质化，在这两个任务中神经网络的诊断准确率与21位皮肤科专科医生相媲美[8]。也有研究表明，除了皮肤镜图像以外，卷积神经网络在大类别皮肤病的普通数码照片样本中也具有良好的分类效果。深度学习方法在医学领域有着广泛的应用前景。

深度学习方法在皮肤痤疮诊断中也有所应用。在2019年Nestlé SHIELD (Skin Health, Innovation, Education, and Longevity Development, NSH)与微软合作开发了一款用于痤疮评估的消费者移动应用程序[9]，它主要根据用户上传的自拍图像评估用户的痤疮严重程度。在数据集上，NSH为这项研究获得了4700张自拍照片，从中剔除曝光过度或曝光不足以及分辨率非常低的图像，并分配给11名皮肤科医生来标注其痤疮严重程度。在数据预处理方面，NSH使用OpenCV检测面部标志，从自拍照中剪切了特定的皮肤贴片（额头、左右脸颊、下巴），并且为了解决卷积神经网络在空间上敏感的问题，采用滚动图像贴片方法将每个皮肤贴片滚动到一定数量的像素上。然后使用ResNet 152[10]从皮肤贴片中提取特征，再输入到一个训练好的3层全连接回归神经网络模型（有1024、512和256个神经元）中，最后输出痤疮严重程度。该基于卷积神经网络的回归模型在区分轻度痤疮的图像上表现良好，但在区分其它几种痤疮严重程度上表现仍然令人不满意。该模型在采用滚动图像贴片方法时引入了额外的噪声，会对检测结果产生影像；该模型的数据集为自拍照片，照片质量层次不齐，照片数目也相对较少，对模型的训练结果也会有较大影响；另外，模型直接对输入的图像判定痤疮严重程度，而未能检测痤疮的位置以及数目等细节信息。

在2019年由Xiaoping Wu、Ni Wen、Jie Liang等人提出了一个统一的痤疮分级深度框架[11]。该框架有两个分支完成痤疮病变计数和痤疮严重程度分级两个任务，先由计数分支预测痤疮病变的分布情况，然后映射到痤疮的严重程度分布上，再由分级分支结合预测的痤疮严重程度分布和映射的严重程度分布来完成对痤疮的分级任务。在计数分支中，论文使用了KL loss来最小化实际目标框和预测目标框的偏差，并分别使用了基于检测和回归的技术方法并进行了比较，例如基于检测的Faster-RCNN[12]方法的效果通常优于基于回归的方法，其达到的最佳准确率高达73.97%，然而其在非常严重等级的痤疮样本中的准确率仅仅只有4.62%，说明Faster-RCNN在每个痤疮类别中很难达到平衡的结果。此外，他们还构建了ACNE04数据集，该数据集是用数码相机收集痤疮病变的图像，所有图像都是从患者的正面以大约70度的角度拍摄所得。数据集包含1457张图像，其中标注了18983个病变的边界框，所有图像都经过专家对皮损数量和痤疮严重程度进行统计和标注。该数据集对后序的痤疮研究工作也有很大的贡献。

### 研究内容

​		本文研究的主要问题是搭建自动检测痤疮模型，依据输入的图像自动标注痤疮所在位置。以VISIA皮肤检测仪收集到的图像作为主要数据集，该图像集为高清图像集，包含不同痤疮严重程度的患者正脸以及侧脸图像。需要对该图像集做出分析，分析痤疮数量、痤疮尺度、所处位置、密集程度等因素，挑选合适的深度学习方法和自动检测框架，并采取合适的改进方案，进一步提升自动检测痤疮模型的检测精度。

​		本文关于搭建自动痤疮模型的具体研究内容如下：

​		1）使用VISIA皮肤检测仪采集不同痤疮严重程度的患者的正脸与侧脸图像，并完成痤疮位置标注工作。针对该数据集，分析痤疮的尺度大小以及相对整体图像的尺度比例、分析痤疮的密集程度和数量等因素，把握图像特点以及需要检测的痤疮目标的特点，并对该图像集进行合理的划分，划分为训练集、测试集、验证集。

​		2）根据图像分析所得到的信息，选取合适的目标检测模型作为基础模型，并结合该模型的特点和图像集信息的特点，在数据增强、训练尺度、锚点框尺寸、骨干网络等方面改进目标检测模型，进一步提升检测精度。

​		3）针对改进前与改进后的目标检测模型，使用由VISIA皮肤检测仪收集的图像划分出的测试集，检测模型精确度和其它各项指标的提升程度；针对最终的目标检测模型，使用别的不同类型的数据集，检测模型在不同数据集中的检测能力。

## 相关原理与技术

### YOLOV3检测算法

- Yolov3网络架构

  ​		darknet-53是yolov3[13]算法的主干网络，yolov3使用了darknet-53的前52层（无全连接层）。yolov3是一个全卷积网络，它摒弃了池化层来降低池化所带来的梯度负面效果，而是通过调节卷积步长来实现下采样，控制输出特征图的尺度大小，也因此yolov3对于输入图像的尺寸大小并没有特别的限制。同时，为了加强算法对小目标的检测精度，yolov3借鉴FPN（feature pyramid networks）[14]的思想，即同时利用低层特征具有高分辨率的特性和高层特征具有高语义信息的特性，通过融合这些低层和高层不同层次的特征信息来达到预测的效果。yolov3的第一张特征图下采样32倍，第二张特征图下采样16倍，第三张特征图下采样8倍，其中小尺度特征图用于检测大尺寸物体，大尺度特征图用于检测小尺寸物体，通过融合这三张不同尺度的特征图来增强检测效果。此外，yolov3结构中还融合了Resnet和Densenet[15]的结构和技巧：使用了concat张量拼接操作，该操作源于DenseNet网络的设计思路，将特征图按照通道维数直接进行拼接操作，从而会扩充张量的维度；引入残差层结构，该结构源于ResNet思想，使用加和操作，即将输入特征层和输出特征层对应维度直接相加而非拼接。

- Yolo输出特征图过程

  ​		yolov3输出的结果就是三张特征图。特征图大小为N * N，其中每个单元格都有3个不同尺度的先验框，每个先验框需要4维来确定位置，需要1维表示检测置信度，K维表示类别概率，因此每个特征图的输出维度为 N * N *(3 * (4 + 1 + K))。

  - 先验框与检测框解码

    ​		在yolov1[16]中，神经网络直接回归输出检测框的宽、高，然而这种方法精度有限且训练较慢。在yolov2和yolov3中就采用先验框，降低了网络的学习难度，大大提升了整体的检测精度。一共有9个先验框，在coco数据集中分别为 (10×13)，(16×30)，(33×23)，(30×61)，(62×45)，(59× 119)， (116 × 90)， (156 × 198)，(373 × 326) ，顺序为w × h。从前往后每三个先验框为一组，分别归属于三张输出特征图，

    ​		得到输出特征图的输出向量以及先验框预先设定的尺寸，就可以得到检测框的位置。检测框的x，y，w，h计算公式如下：
    $$
    b_x = \sigma(t_x) + c_x	\\
    b_y = \sigma(t_y) + c_y	\\
    b_w = p_we^{t_w}	\\
    b_h = p_he^{t_h}
    $$
    

    其中c_x、c_y为cell左上角坐标，t_x、t_y、t_w、t_h为特征图输出中关于先验框信息的4维向量，p_w、p_h为先验框预先设定的宽和高，b_x、b_y、b_w、b_h为检测框的x、y、w、h，\sigma为激活函数，在yolov3中使用的是sigmoid函数。

  - 置信度解码

    每个先验框都需要一维表示置信度。置信度包含两重信息：第一重信息是当前预测框是否包含对象的概率，即判定当前预测框内是物体还是背景；第二重信息是当前预测框若有物体，则预测框和物体真实框可能的IOU(Intersection over Union)交互比。关于置信度的计算公式如下：

    ​	Cij=Pr(Object)∗IOUpredtruth

    其中，Pr表示预测该预测框中是否存在物体，存在为1，不存在则为0，IOU则为预测框和物体真实框的交互比，置信度则为这二者的乘积。

  - 类别解码

    每个先验框都需要K维表示类别概率，K为检测物体的种类数目。K维向量的意义是当预测框包含物体时，该物体为要检测的所有类别中每种类别的概率。在yolov3中使用了sigmoid函数代替v2中的softmax函数，从而使网络更加灵活，取消了类别之间的互斥。

- 损失计算

  对于每个预测框都会将其分为三类：正样本、负样本、不参与计算样本。对于每个样本都有不同的损失计算方法，正负样本的选择方法如下：

  1. 正样本的选择

     首先计算真实框的中心落在哪一个单元格中，然后计算这个单元格中所有检测框和目标真实框的交互比，选取交互比值最大的检测框和真实框匹配，于是该检测框负责预测该真实框，其余的单元格、检测框都不负责该真实框。

  2. 负样本的选择

     计算每个预测框和所有目标真实框之间的交互比，如果某个预测框和图像中所有真实框的最大交互比小于某个阈值（一般为0.5），那么认定该预测框不包含目标，记作负样本，同时其置信度应当为0.

  3. 不参与计算样本

     该预测框不负责预测对象，但是和图像中真实框的交互比大于某个阈值（一般为0.5），则可以认为该预测框包含了目标的一部分，不可以简单当作负样本，所以将这个预测框当作不参与计算样本。

  对于正样本需要计算置信损失、分类损失、定位损失，对于负样本只需要计算置信损失，而不参与计算样本不需要计算损失。

  置信度损失：参考ppt

  其中o_i在0~1之间，表示预测目标边界框与真实目标边界框得交互比。c为预测置信度值，c尖为c通过sigmoid函数后得到得预测置信度，N为正负样本个数。

  类别损失：参考ppt

  其中O_ij取值为0、1，表示预测目标边界框i中是否存在第j类目标。C_ij为预测类别概率值，C_ij尖表示C_ij通过sigmoid函数得到得目标概率。N_pos为正样本个数。

  定位概率：参考ppt

### YOLOV3-spp

- spp模块

  yolov3-spp中的spp模块，是受spp-net（Spatial Pyramid Pooling）[17]的启发而来，在yolov3卷积过程中加入一个spp模块，该模块由4个并行的分支构成，分别由步长为1的5*5池化层、步长为1的9\*9池化层、步长为1的13 * 13的池化层和一个跳跃连接构成，将这四个分支输出的特征图再通过concat拼接操作生成一张新的特征图传入下一层网络中。该spp模块借鉴了空间金字塔的思想，将局部特征和全局特征进行融合，丰富了特征图的表达能力，也有利于检测目标尺度大小差异大的情况，因此使得模型的检测精度提升巨大。

- focal loss

  focal loss[18]解决的问题主要是在一阶段检测方法中，一张图像中能够匹配到目标的预测框（正样本）个数一般只有十几个到几十个，而没有匹配到的预测框（负样本）数量远大于匹配到的，大概在10^4 - 10^5个。而这些未匹配到目标的预测框大多为简单易分负样本（对训练网络起不到作用），但由于数量过多往往会掩盖那些少量但对训练网络有帮助的样本。focal loss的计算公式如下：

  \gamma因子用于削减易分类样本的损失，使得模型更关注于困难的、有助于训练网络的样本。加入\alpha平衡因子，用于平衡正负样本的比例。
  

### K-means聚类算法

标准的K-means[19]算法是一种简单的无监督学习算法，目的是将数据集划分为K簇，从而使同一簇间的数据相似性高，不同簇间的数据相似性低。其中数据可以看作为数据点，相似性越高意味着数据点间距离越近，而簇就是数据点的集合。具体算法步骤如下：

1. 确定一个K值，即数据集簇的个数；
2. 在数据集中随机选取K个数据点作为K个簇中心；
3. 对数据集中的每个数据点，计算其与K个簇中心的距离（例如欧式距离），距离哪个簇最近就将该数据点分配给该簇；
4. 计算每个簇中所有数据点的均值，作为该簇新的中心
5. 重复3、4步骤，直到均簇中心不再变化或者达到最大迭代数；

对先验框进行K-means算法时我们还需要进行度量选择。通常，先验框由(x1,y1,x2,y2)表示，(x1,y1)为左上角顶点，(x2,y2)为右下角顶。对先验框做聚类时，我们仅需要先验框的宽高作为特征，即以宽和高作为数据点的坐标。又因为数据集中图像尺寸不一，因此需要先对先验框相对于图像做归一化处理，即
$$
w = w_{box}/w_{img},h=h_{box}/h_{img}
$$
同时对相似性的度量我们也需要做出修改，并不能直接使用欧式距离作为相似性指标，而应该关心先验框与目标框的IOU（Intersection over Union），以IOU作为相似性的度量。假设先验框尺寸为(w_a,h_a)，目标框为(w_b,h_b)，计算IOU时将先验框和目标框左上角顶点重合，则IOU的计算公式如下：
$$
IOU(box,anchor) = \frac{intersection(box,anchor)}{union(box,anchor)-intersection(box,anchor)} \\
=\frac{min(w_a,w_b)\cdot min(h_a,h_b)}{w_ah_a+w_bh_b-min(w_a,w_b)\cdot min(h_a,h_b)}
$$
IOU的取值范围为[0,1]，且IOU值越大表示先验框和目标框间的相似性越高，则对应的数据点距离应该越近，因此最终相似性度量公式如下：
$$
d(box,anchor) = 1 - IOU(box,anchor)
$$
因此对先验框做K-means的步骤为：

1. 确定先验框个数K

2. 在目标框集中随机选取K个目标框作为初始先验框；

3. 对目标框集中的每个目标框，计算其与K个先验框的距离（1-IOU），距离哪个先验框最近就将该目标框分配给该先验框；

4. 计算每个簇中所有框宽和高的均值，作为该簇新的先验框

5. 重复3、4步骤，直到先验框尺寸不再变化或者达到最大迭代数；

## 基于VISIA的自动痤疮检测模型搭建

### 数据采集与分析

由VISIA皮肤检测仪采集得到1075位痤疮患者正侧脸图像，每位患者正脸采集一张，左右侧脸各一张。1075位患者中，Ⅰ级痤疮严重程度227位，Ⅱ级386位，Ⅲ级266位，Ⅳ级196位，痤疮分级标准依照Pillsbury 分级法。由专业皮肤科医生对采集照片进行标注，均用矩形框框定痤疮位置以及痤疮种类，痤疮种类分为粉刺、丘疹、脓疱、结节、囊肿、瘢痕六类。

经统计，每张采集得患者照片大小在2000 * 2800到3456 * 5184之间，图像拍摄清晰度高。其中标注出的痤疮区域大小在20 * 22到1080 * 269之间。可以看出单幅图像分辨率较高，而痤疮目标尺度较小，依据超小目标为整个图像0.12%以下的目标的定义，绝大多数痤疮目标都为超小目标。而对于不同的痤疮目标，其目标尺度变化较大。对每个痤疮目标，其宽高比最小为0.116，最大为28，总计痤疮目标为19765，其中宽高比介于0.8到1.2之间的有13339个，宽高比小于0.5以及大于1.5的有1020个，可见痤疮目标宽高比多变。

我们筛选了其中885张患者正脸图像作为数据集（由于某些正脸照片并无痤疮标注，因此剔除了部分照片）。以4:1比例划分训练集和测试集，训练集包含708张图像，总痤疮数为4395个。测试集包含177张图像，总痤疮数为1066个。各类痤疮分布如下：

| 囊肿 | 粉刺 | 结节 | 丘疹 | 脓疱 | 瘢痕 | 总和 |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| 27   | 1078 | 190  | 2050 | 747  | 303  | 4395 |

​						 表1  训练集痤疮种类分布

| 囊肿 | 粉刺 | 结节 | 丘疹 | 脓疱 | 瘢痕 | 总和 |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| 5    | 294  | 39   | 482  | 159  | 87   | 1066 |

​								表2  测试集痤疮种类分布

并且，我们对训练集和测试集中每张图像所包含痤疮目标数目做了如下的统计：

 图1  训练集痤疮种类分布

 图2  测试集痤疮种类分布

可以看到，无论是对训练集还是测试集，绝大多数图像包含痤疮数小于10，分布是比较稀疏的。然而也存在部分图像包含几十甚至上百个痤疮目标，其痤疮分布相对密集。

综上，由VISIA皮肤检测仪采集并标注后的数据集，其原图尺度大，而检测目标多为小目标，且尺度变化大，宽高比多样，分布稀疏和密集情况并存。

### 模型搭建与优化

本论文自动痤疮检测模型以yolov3模型为基础。yolov3为速度和精度均衡的目标检测网络，重点解决了小物体检测问题。yolov3不仅引入了FPN结构，同时它的检测层由三张特征图融合，这样的结构在检测小物体上有相对较好的性能。本论文选用yolov3-spp目标检测网络作为基础模型，输入图像尺寸为640 * 640，训练轮数epochs为300轮，输入网络批次数量batch-size为16。数据集为885张患者正脸图像，708张作为训练集，177张作为测试集，测试集中44张图像作为验证集。在yolov3的基础上，我们做了相应的改进以进一步提升其性能。

#### 图像增强

采用mosaic数据增强[23]方法，即每次读取四张图像，并对这四张图像进行翻转操作（左右翻转）、缩放操作、色域变化操作（明亮度、饱和度、色调变化）。操作完后将第一张图像摆放在左上角，第二张摆放在左下角，第三张摆放在右下角，第四张摆放在右上角，完成摆放后再进行图像的组合和标注框的组合，将四张图拼接为一张图。

通过mosaic，可以丰富图像的背景，增加目标地多样性，增加一张图像的目标个数，同时将四张图像拼接为一张图像变相地提高了batch_size，从而提高内存的利用率、提高训练速度、增加梯度下降方向准确度。

#### 图像分块操作

由于原图像分辨率过大（最大为3456 * 5184）而检测目标尺度小（最小为20 * 22），若直接输入目标检测网络（输入尺寸最大为640 * 640），则会对原图进行下采样而可能造成数据的丢失，因此我们需要对原输入图像进行切块操作，分别输入模型再进行检测。切块方法可以分为离线切图和在线切图：离线切图为训练前把原数据集直接切块，再输入网络中训练；在线切图为在训练时调用RandomCrop、RandomCenterCropPad等函数，边切图边训练。由于在线切图训练时间长，且GPU利用率可能比较低，因此我使用离线切图方法。

采用滑动窗口切图方法，以640 * 640的滑动窗口进行剪切，剪切方向自左向右，自上而下，并且将滑动步长设为400。由于滑动步长为400，滑动过程中滑动窗口间保留了一定的交叠，防止在滑动过程中遗漏某些比较大的目标（虽然有一定交叠，仍然会舍去某些目标框，但该类目标框全为对某些密集痤疮区域的笼统标注，对于我们检测细粒度痤疮没有帮助，因此可以舍去）。由于某些图像的目标框小且稀疏，因此滑动过程中需要排除没有目标框的切图，否则会大量不包含目标框的图像分块而大大降低训练速度。

对原数据集的885张正脸图像采用滑动窗口剪切，得到7118张图像分块，再按照4:1比例划分训练集和测试集，得到训练集5695张图像，测试集1423张图像。

#### 多尺度训练

由于yolov3-spp模型只包含卷积层和池化层，因此可以随时改变输入图像的尺寸来提高网络的适应性。具体做法是，检测网络每经过10个批次（10 batch-size）训练就会随机选择新的图像尺寸大小（320，352，...，640，672，...，都为32的倍数，因为检测网络的下采样因子为32），再输入到图像中进行训练，从而提高检测网络对不同尺寸图象的鲁棒性，但可能会降低模型的训练速度。

#### 聚类方法调整先验框

由于我们的yolov3-spp网络依靠先验框来定位检测框的位置和大小，先验框的大小也决定了我们检测网络感受野大小以及检测相应目标的能力。而我们原模型的先验框（anchor）大小设置来源于coco数据集[20]，其设置参数为(10×13)，(16×30)，(33×23)，(30×61)，(62×45)，(59× 119)， (116 × 90)， (156 × 198)，(373 × 326)，其大小和宽高比针对的是coco数据集中的检测目标，不一定适合我们的痤疮检测数据集。由之前数据采集与分析可知，对每个痤疮目标，其宽高比最小为0.116，其中宽高比介于0.8到1.2之间的有13339个，宽高比小于0.5以及大于1.5的有1020个，可见痤疮目标宽高比多变，因此我们使用K-means聚类算法重新生成先验框，新生成的先验框为(30×28)，(44×42)，(59×58)，(69×78)，(86×72)，(98× 95)， (118 × 122)， (159 × 156)，(223 × 228)。

## 实验分析

针对改进前的基础痤疮检测模型与改进后的检测模型（使用图像分块操作、聚类方法调整先验框等），使用VISIA皮肤检测仪采集得到的数据集进行测试，检测模型精确度和其它各项指标的提升程度；

针对最终的痤疮目标检测模型，分别使用不同类型的数据集，检测模型在不同数据集中的检测能力，评价最终的痤疮目标检测模型效果。

### 实验目的与实验内容

本实验的目的主要在于通过测试不同改进方法（图像分块操作、聚类方法调整先验框等）对痤疮目标检测模型检测能力的提升程度，找到一个优化过的痤疮检测模型使其检测效果达到最优。并且对该最优痤疮检测模型使用不同类型的数据集进行测试（其中有使用VISIA皮肤检测仪采集的图像，也有使用其它设备采集的图像），从而客观的评价该检测模型的检测效果以及鲁棒性。

实验内容主要分为以下：

一是检测图像分块操作对模型检测效果的提升能力。分别使用原始数据集和经过分块操作后的数据集，训练基础痤疮目标检测模型。并用分块后的正脸数据集、分块后的侧脸数据集分别检测两个模型的检测效果；

二是检测调整先验框大小对模型检测效果的提升能力。使用经过分块操作后的数据集，分别训练基础痤疮目标检测模型和更改先验框大小后的痤疮目标检测模型，并用分块后的正脸数据集、分块后的侧脸数据集分别检测两个模型的检测效果；

三是检测得到的最优痤疮检测模型的检测效果和鲁棒性。分别使用经过分块操作后的数据集和南开大学的ACNE04数据集（也经过分块处理）来检测我们的痤疮检测模型的检测效果。

### 数据集与评价指标

正脸数据集：由VISIA皮肤检测仪采集得到1075位痤疮患者正脸图像，并剔除没有标注的图像得到885张患者正脸图像作为正脸数据集。所有正脸图像都为尺度较大图像，其大小在2000 * 2800到3456 * 5184之间，大部分痤疮相对于图像算作超小目标。按照4:1比例划分训练集和测试集，测试集按照4:1划分测试集和验证集，训练集包含708张图像，测试集包含133张图像，验证集包含44张图像。

正脸分块数据集：将正脸数据集图像经过步长400，滑动窗口大小为640 * 640 的滑动窗口剪切得到7118张图像分块，此时痤疮相对于图像不再是超小目标。该7118张图像分块即为正脸分块数据集。再按照4:1比例划分训练集和测试集，测试集按照4:1划分测试集和验证集，得到训练集5695张图像，测试集1138张图像，验证集285张。

侧脸分块数据集：将侧脸数据集图像（共1995张图像）经过步长400，滑动窗口大小为640 * 640 的滑动窗口剪切得到16744张图像分块，该16744张图像分块即为侧脸分块数据集。

南开大学ACNE04数据集：该数据集是用数码相机收集痤疮病变的图像，所有图像都是从患者的正面以大约70度的角度拍摄所得。数据集包含1457张图像，其中标注了18983个痤疮病变的边界框，所有图像都经过专家对皮损数量和痤疮严重程度进行统计和标注。该1457张图像即为ACNE04数据集。

ACNE04分块数据集：由原ACNE04数据集经过步长400，滑动窗口大小为640 * 640 的滑动窗口剪切得到11319张图像分块，该11319张图像分块即为ACNE04分块数据集。

准确率（Precision），又称为查准率，表示预测为正样本的样本中，正确预测为正样本的概率;

召回率（Recall），又称为查全率，表示那些在原始样本的正样本中，最后被正确预测为正样本的概率;

漏检率，其值为1-Recall，表示那些被判断错误的正例的概率，即所有正例中漏掉的正例所占的比例;

P-R曲线，就是关于准确率（Precision）和召回率（Recall）的曲线，通过改变是否检测到物体的阈值（通常为检测框与真实框的IOU值）同时改变准确率和召回率，从而得到曲线。一个性能好的检测器，在召回率提高时也可以保持准确率在很高的水平。而一个性能差的检测器可能大量损失准确率来提高召回率值的提高;

平均精度（Average-Precision，AP），为P-R曲线所围成的面积，通常平均精度值越高代表检测器性能越好;

平均AP值（mean Average-Precision），为多个类别AP值的平均;

mAP@.5与mAP@.95，表示检测到物体的阈值（即检测框与真实框的IOU值）为0.5和0.95时mAP的取值;

### 分块操作实验

我们的实验模型选用yolov3-spp目标检测网络，该网络的输入图像大小为640 * 640，并对图像做mosaic数据增强，开启多尺度训练方法，并使用单种类检测方法（将6钟痤疮种类看作一种进行检测），模型先验框为预设先验框大小。图像训练轮数epochs为300轮，训练批次batch-size为16，训练优化器optimizer为SGD（Stochastic Gradient Descent）[21]，初始学习率为0.01，学习动量momentum为0.937，权重衰减weight decay为0.0005。我们的目标检测网络运行在一个 GeForce RTX 2080 Ti GPU上，并且是基于PyTorch框架来实现的。

#### 正脸数据集训练模型

首先直接使用正脸数据集中的训练集来训练模型，训练集包含708张正脸图像。该模型在训练时对验证集的最优检测结果如下表所示：

| 准确率 | 召回率 | 漏检率 | mAP@.5 | mAP@.95 |
| ------ | ------ | ------ | ------ | ------- |
| 0.3334 | 0.2286 | 0.7714 | 0.1785 | 0.0606  |

​														表：对验证集的检测结果

使用上述模型对正脸数据集中的测试集做测试，测试集包含141张正脸图像。测试时，输入图像大小为640 * 640，测试批次batch-size为32，物体置信度阈值为0.001，NMS（Non-Maximum Suppression）[22]的IOU阈值为0.6，并且我们对测试数据做图像增强操作，得到如下测试结果：

|            | 准确率     | 召回率     | 漏检率     | mAP@.5     | mAP@.95     |
| ---------- | ---------- | ---------- | ---------- | ---------- | ----------- |
| 数据增强后 | 0.251      | 0.452      | 0.548      | 0.283      | 0.106       |
|            | **准确率** | **召回率** | **漏检率** | **mAP@.5** | **mAP@.95** |
| 数据增强前 | 0.332      | 0.412      | 0.588      | 0.27       | 0.101       |

​														表：对正脸测试集的检测结果	

上述模型采取数据增强后对正脸数据集的P-R曲线和AP值如下图所示：

![precision_recall_curve](E:\PythonProject\yolo\yolov3\runs\test\yolo-spp--testFront\precision_recall_curve.png)

​									图：采用数据增强后的正脸数据集检测结果

使用上述模型对正脸分块数据集中的测试集做检测，测试集中包含1138张正脸分块图像。得到如下检测结果：

|            | 准确率     | 召回率     | 漏检率     | mAP@.5     | mAP@.95     |
| ---------- | ---------- | ---------- | ---------- | ---------- | ----------- |
| 数据增强后 | 0.156      | 0.113      | 0.887      | 0.0502     | 0.0136      |
|            | **准确率** | **召回率** | **漏检率** | **mAP@.5** | **mAP@.95** |
| 数据增强前 | 0.137      | 0.0391     | 0.9609     | 00191      | 0.00676     |

​														表：对正脸分块测试集的检测结果								

上述模型采取数据增强后对正脸分块数据集测试的P-R曲线和AP值如下图所示：

![precision_recall_curve](E:\PythonProject\yolo\yolov3\runs\test\yolo-spp--testFrontCut\precision_recall_curve.png)

​					图：采用数据增强后的正脸数据集检测结果

#### 分块数据集训练模型

我们使用正脸分块数据集来训练模型，训练集中包含5695张正脸分块图像。该模型在训练时对验证集的最优检测结果如下表所示：

| 准确率 | 召回率 | 漏检率 | mAP@.5 | mAP@.95 |
| ------ | ------ | ------ | ------ | ------- |
| 0.8287 | 0.8238 | 0.1762 | 0.8375 | 0.6698  |

​														表：对验证集的检测结果

使用上述模型对正脸分块数据集中的测试集做测试，测试集包含1138张正脸分块图像。测试时，输入图像大小为640 * 640，测试批次batch-size为32，物体置信度阈值为0.001，NMS的IOU阈值为0.6，并且我们对测试数据做图像增强操作，得到如下测试结果：

|            | 准确率     | 召回率     | 漏检率     | mAP@.5     | mAP@.95     |
| ---------- | ---------- | ---------- | ---------- | ---------- | ----------- |
| 数据增强后 | 0.723      | 0.841      | 0.159      | 0.841      | 0.847       |
|            | **准确率** | **召回率** | **漏检率** | **mAP@.5** | **mAP@.95** |
| 数据增强前 | 0.812      | 0.814      | 0.186      | 0.827      | 0.675       |

​													    表：对正脸分块测试集的检测结果

上述模型采取数据增强后对正脸分块数据集测试的P-R曲线和AP值如下图所示：

![precision_recall_curve](E:\PythonProject\yolo\yolov3\runs\test\yolo-spp-cut-testFrontCutAug\precision_recall_curve.png)

​								图：采用数据增强后的正脸分块数据集检测结果

#### 结果分析

直接由正脸数据集训练得到的痤疮目标检测模型在正脸数据集的测试集和正脸分块数据集的测试集上表现效果差，其准确率、召回率、平均精度都偏低，同时有较高的漏检率，漏检率高达0.887，特别是在正脸分块数据集上的预测，对大部分痤疮目标都无法检出。这都是由于原图像分辨率过大而痤疮目标尺寸较小，直接输入目标检测网络中需要下采样而导致过多信息的丢失。而通过将正脸数据集进行分块操作得到正脸分块数据集，使得新的数据集在尺寸上适应目标检测模型的输入尺寸，从而使用新的数据集训练得到的模型在准确率、召回率、平均精度上都远远优于直接由正脸数据集训练得到的模型，并且漏检率大大降低，检测效果提升显著。并且由于数据集中某些痤疮目标医生并未标出，实际准确率相比得到的准确率要偏高，该痤疮目标检测模型可以正确检测出分块数据集中的绝大部分痤疮目标。

另外，我们可以发现对检测进行数据增强操作能进一步提高我们痤疮目标检测模型的检测效果。虽然准确率会有一定的下降，但可以进一步提升召回率和漏检率。其中准确率的下降很大一部分原因是由于皮肤科医生在对数据进行标注时忽略了一些痤疮目标，而我们更关注的是漏检率，宁可检错也要尽量降低我们的漏检率。因此召回率和漏检率的提升也是对我们的目标检测模型检测效果的提升。数据增强在我们的痤疮目标检测中是很有效的。

### 调整先验框实验

同样的，我们的实验模型选用yolov3-spp目标检测网络，输入图像大小为640 * 640，并对图像做mosaic数据增强，开启多尺度训练方法，并使用单种类检测方法。我们起始先验框大小为(10×13)，(16×30)，(33×23)，(30×61)，(62×45)，(59× 119)， (116 × 90)， (156 × 198)，(373 × 326)，经过K-means聚类算法修改后为(30×28)，(44×42)，(59×58)，(69×78)，(86×72)，(98× 95)， (118 × 122)， (159 × 156)，(223 × 228)。训练轮数、训练批次、优化器、学习率等参数与分块操作实验无异。

#### 调整先验框训练模型

们使用正脸分块数据集来训练模型，训练集中包含5695张正脸分块图像。该模型在训练时对验证集的最优检测结果如下表所示：

| 准确率 | 召回率 | 漏检率 | mAP@.5 | mAP@.95 |
| ------ | ------ | ------ | ------ | ------- |
| 0.8307 | 0.8213 | 0.1787 | 0.8383 | 0.6877  |

​														表：对验证集的检测结果

使用上述模型对正脸分块数据集中的测试集做测试，测试集包含1138张正脸分块图像。测试时，输入图像大小为640 * 640，测试批次batch-size为32，物体置信度阈值为0.001，NMS的IOU阈值为0.6，并且我们对测试数据做图像增强操作，得到如下测试结果：

|            | 准确率     | 召回率     | 漏检率     | mAP@.5     | mAP@.95     |
| ---------- | ---------- | ---------- | ---------- | ---------- | ----------- |
| 数据增强后 | 0.774      | 0.837      | 0.163      | 0.851      | 0.706       |
|            | **准确率** | **召回率** | **漏检率** | **mAP@.5** | **mAP@.95** |
| 数据增强前 | 0.86       | 0.812      | 0.188      | 0.831      | 0.697       |

​													    表：对正脸分块测试集的检测结果

上述模型采取数据增强后对正脸分块数据集测试的P-R曲线和AP值如下图所示：

![precision_recall_curve](E:\PythonProject\yolo\yolov3\runs\test\yolo-spp-changeAnchor-testFrontCutAug\precision_recall_curve.png)

​								图：采用数据增强后的正脸分块数据集检测结果

#### 结果分析

调整先验框大小后重新训练模型，并对测试数据做数据增强处理，可以发现我们的痤疮目标检测模型准确率相比使用起始先验框的目标检测模型的准确率有所提升，由0.723提升到0.774，召回率有小幅度下降，由0.841下降到0.837，而漏检率有小幅度上升，由0.159上升到0.163，mAP@.5值相对上升了0.01，mAP@.95值下降明显，相对下降0.24。

同时，对测试数据的数据增强处理对该目标检测模型的准确率影响巨大，采用数据增强后，由原0.86的检测准确率降低为0.744，而召回率升高了0.025，漏检率降低了0.025。

### ACNE04数据集检测实验

ACNE04数据集其尺寸在3000 * 3000左右，因此我们需要对其进行分块操作，由原1457张图像分块得到11319张分块图像作为测试数据集合。本实验我们使用由正脸分块数据集训练得到的痤疮检测模型、正脸分块数据集训练并更改先验框大小的痤疮检测模型两个模型来测试ACNE04数据集。更改先验框的痤疮检测模型相较于未更改的在准确率上更占优，然而在召回率、漏检率、mAP@.95上性能有所下降，而这两个模型的检测效果要远优于由正脸数据集训练得到的模型，因此我们选用以上两个痤疮检测模型测试ACNE04数据集。

未调整先验框的痤疮目标检测模型测试结果如下：

|            | 准确率     | 召回率     | 漏检率     | mAP@.5     | mAP@.95     |
| ---------- | ---------- | ---------- | ---------- | ---------- | ----------- |
| 数据增强后 | 0.628      | 0.821      | 0.179      | 0.806      | 0.556       |
|            | **准确率** | **召回率** | **漏检率** | **mAP@.5** | **mAP@.95** |
| 数据增强前 | 0.741      | 0.79       | 0.21       | 0.796      | 0.535       |

​													    表：对ACNE04分块测试集的检测结果

上述模型采取数据增强后对ACNE04分块数据集测试的P-R曲线和AP值如下图所示：

![precision_recall_curve](E:\PythonProject\yolo\yolov3\runs\test\yolov3-changeAnchor-trainNankaiAndFrontCut-testNanKaiAug\precision_recall_curve.png)

​								图：采用数据增强后的ACNE04分块数据集检测结果

调整先验框的痤疮目标检测模型测试结果如下：

|            | 准确率     | 召回率     | 漏检率     | mAP@.5     | mAP@.95     |
| ---------- | ---------- | ---------- | ---------- | ---------- | ----------- |
| 数据增强后 | 0.718      | 0.806      | 0.194      | 0.809      | 0.621       |
|            | **准确率** | **召回率** | **漏检率** | **mAP@.5** | **mAP@.95** |
| 数据增强前 | 0.809      | 0.785      | 0.215      | 0.797      | 0.605       |

​													    表：对ACNE04分块测试集的检测结果

上述模型采取数据增强后对ACNE04分块数据集测试的P-R曲线和AP值如下图所示：

![precision_recall_curve](E:\PythonProject\yolo\yolov3\runs\test\yolov3-changeAnchor-trainNankaiAndFrontCutAgain-testNanKaiAug\precision_recall_curve.png)

为了验证痤疮目标检测系统的实用价值，上述两个痤疮目标检测模型还与2位专业皮肤科医生和2位普通医生进行比较，比较结果如下：

|        | GD1   | GD2   | Derm1 | Derm2 | Acne | Acne-Aug | Acne-Anchor | Acne-Anchor-Aug |
| ------ | ----- | ----- | ----- | ----- | ---- | -------- | ----------- | --------------- |
| 准确率 | 62.87 | 62.07 | 77.33 | 82.96 | 74.1 | 62.8     | 80.9        | 71.8            |
| 召回率 | 55.27 | 68.33 | 72.56 | 78.27 | 79   | 82.1     | 78.5        | 80.6            |
| 漏检率 | 44.73 | 31.67 | 27.44 | 21.73 | 21   | 17.9     | 21.5        | 19.4            |

​						图：在ACNE04数据集上与医生的比较。Derm是指在皮肤科领域有知识的专业皮肤科医生，GD是指不专攻皮肤科的普通医生，Acne为选择的痤疮目标检测模型，Aug为对测试集采用数据增强，Anchor为修改了先验框

#### 结果分析

首先我们可以观察到，皮肤科领域的专业知识在痤疮检测中起着十分重要的作用。不专攻皮肤科的普通医生在对痤疮检测过程中的准确率、召回率、漏检率上表现十分平庸，而在皮肤科领域有知识的专业皮肤科医生在所有指标上都有更好的表现。我们的模型相较于不专攻皮肤科的普通医生是远远超过的，在各项指标上都是远优于的。而相较于专业的皮肤科医生，在准确率这项指标上还是相对较低的，除了更改先验框大小并且不采用测试集数据增强的这组痤疮检测模型在准确率上达到了专业皮肤科医生的平均性能， 其它几组痤疮检测模型的准确率都要低于专业皮肤科医生。然而在召回率、漏检率指标上我们的模型表现都是优秀的，几组模型的召回率和漏检率都持平甚至优于专业皮肤科医生的检测指标，其中不更改先验框大小并且采用测试集数据增强的这组痤疮检测模型在召回率上达到了82.1%，超过专业皮肤科医生4%。

同时，对测试集的数据增强在该实验中影响仍然很大，特别是会导致准确率的大幅度降低。对不更改先验框的痤疮检测模型，采用对测试集的数据增强后，其准确率下降了12.2%。而对更改先验框的痤疮检测模型，采用对测试集的数据增强后其准确率也下降了9.1%。而两个模型在采用测试集数据增强后召回率仅仅上升3%左右，因此对测试集采用数据增强需要谨慎使用。

## 总结与展望





[1] V, Goulden, and, et al. Prevalence of facial acne in adults - ScienceDirect[J]. Journal of the American Academy of Dermatology, 1999, 41(4):577-580.
[2] Nobukazu, HAYASHI, Hirohiko, et al. Establishment of grading criteria for acne severity[J]. The Journal of Dermatology, 2008, 35(5):255-260.
[3] 付俊, 宋璞, 王延婷,等. VISIA皮肤测试仪对多种皮肤特征的定量评价[C]// 美丽人生 和谐世界——中华医学会第七次全国医学美学与美容学术年会、中华医学会医学美学与美容学分会20周年庆典暨第三届两岸四地美容医学学术论坛. 0.
[4] Dalal N . Histograms of oriented gradients for human detection[J]. Proc of Cvpr, 2005.
[5] Chih-Chung, Chang, Chih-Jen, et al. LIBSVM: A library for support vector machines[J]. Acm Transactions on Intelligent Systems & Technology, 2011.
[6] Felzenszwalb P F ,  Mcallester D A ,  Ramanan D . A discriminatively trained, multiscale, deformable part model[C]// 2008 IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2008.
[7] Alamdari N ,  Tavakolian K ,  Alhashim M , et al. Detection and classification of acne lesions in acne patients: A mobile application[C]// IEEE International Conference on Electro Information Technology. IEEE, 2016.
[8] Andre E ,  Brett K ,  Novoa R A , et al. Dermatologist-level classification of skin cancer with deep neural networks[J]. Nature, 2019, 2017年542卷7639期:115-118页.
[9] Zhao T ,  Zhang H ,  Spoelstra J . A Computer Vision Application for Assessing Facial Acne Severity from Selfie Images[J].  2019.
[10] He K ,  Zhang X ,  Ren S , et al. Deep Residual Learning for Image Recognition[J]. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
[11] Wu X ,  Wen N ,  J  Liang, et al. Joint Acne Image Grading and Counting via Label Distribution Learning[C]// International Conference on Computer Vision. 0.
[12] Ren S ,  He K ,  Girshick R , et al. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks[J]. IEEE Transactions on Pattern Analysis & Machine Intelligence, 2017, 39(6):1137-1149.
[13] Redmon J ,  Farhadi A . YOLOv3: An Incremental Improvement[J]. arXiv e-prints, 2018.

[14] Lin T Y ,  Dollar P ,  Girshick R , et al. Feature Pyramid Networks for Object Detection[C]// 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Computer Society, 2017.
[15] Huang G ,  Liu Z ,  Laurens V , et al. Densely Connected Convolutional Networks[C]// IEEE Computer Society. IEEE Computer Society, 2016.
[16] Redmon J ,  Divvala S ,  Girshick R , et al. You Only Look Once: Unified, Real-Time Object Detection[J]. IEEE, 2016.
[17] He K ,  Zhang X ,  Ren S , et al. Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition[J]. IEEE Transactions on Pattern Analysis & Machine Intelligence, 2014, 37(9):1904-16.
[18] Lin T Y ,  Goyal P ,  Girshick R , et al. Focal Loss for Dense Object Detection[J]. IEEE Transactions on Pattern Analysis & Machine Intelligence, 2017, PP(99):2999-3007.
[19] Wong J A H A . Algorithm AS 136: A K-Means Clustering Algorithm[J]. Journal of the Royal Statistical Society, 1979, 28(1):100-108.
[20] Lin T Y ,  Maire M ,  Belongie S , et al. Microsoft COCO: Common Objects in Context[J]. European Conference on Computer Vision, 2014.

[21] Bottou L . Stochastic Gradient Descent Tricks[M]. Springer Berlin Heidelberg, 2012.
[22] Neubeck A ,  Gool L J V . Efficient Non-Maximum Suppression[C]// International Conference on Pattern Recognition. IEEE Computer Society, 2006.

[23] Bochkovskiy A ,  Wang C Y ,  Liao H . YOLOv4: Optimal Speed and Accuracy of Object Detection[J].  2020.