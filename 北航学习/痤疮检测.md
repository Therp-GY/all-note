# 痤疮检测

[关于人工智能，机器学习，深度学习的综述](https://www.zhihu.com/question/57770020)

机器学习是实现人工智能的方法，深度学习是机器学习的一种，深度学习主要是用深度神经网络来解决问题

[关于人脸检测的综述](https://zhuanlan.zhihu.com/p/56414557)

[有意思的例子](https://www.zhihu.com/search?type=content&q=%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E5%85%A5%E9%97%A8)

## 相关文章与竞赛

https://workshop2019.isic-archive.com

https://challenge2019.isic-archive.com/

- Constructing Bias on Skin Lesion Datasets
  
- 相关数据集
  
- Solo or Ensemble? Choosing a CNN Architecture for Melanoma Classification

  - 更好的图像网络模型可以transfer 更好

  - 如何预测模型的表现

    9个模型、5个划分、3个重复

- A Deep Attention Model for the Hiearchical Diagnosis of Skin Lesions

- Skin Lesion Classification Using Loss Balancing and Ensembles of Multi-Resolution EfficientNets

  - 流程

    1. 图像剪裁

       > 作为第一步，我们使用裁剪策略来处理未裁剪的图像
       >
       > 通常会出现大面积的黑色区域。我们用很低的分辨率对图像进行二值化
       >
       > 阈值，使整个皮肤镜视野设置为1。然后，我们找到一个椭圆的质心和长轴和短轴，这个椭圆的第二中心矩和内部区域相同。基于这些值，我们导出了一个矩形边界框，用于覆盖相关视野的裁剪。这些过程如图1所示。我们自动决定必要性
       >
       > 对于基于启发式的裁剪，该启发式测试边界框内的平均强度是否与边界框外的平均强度有实质性差异。人工检验表明，该方法具有较强的鲁棒性。在训练集中，6226个被自动裁剪。在测试集中，3864个图像被自动裁剪。接下来，我们应用明可夫斯基范数p=6的灰度恒常性方法，紧随去年的优胜者[3]。这一点尤其重要，因为用于培训的数据集差别很大。此外，我们调整了数据集中较大的图像的大小。我们以HAM10000分辨率为参考，在保持纵横比的同时，将所有图像的长边调整为600像素。

    2. 模型选择

       > 体系结构。我们在很大程度上依赖于EfficientNets（EN）[13]，这些EfficientNets（EN）[13]已经通过AutoAugment v0策略在ImageNet数据集上进行了预训练[6]。此模型族包含八个不同的模型，这些模型在结构上相似，并遵循一定的缩放规则以调整为更大的图像大小。最小版本B0使用标准输入尺寸224×224。更大的版本，高达B7，使用更大的输入大小，同时也扩大了网络宽度（每层的功能4 Nils-Gessert等人地图的数量）和网络深度（层的数量）。我们采用EN B0到B6。为了在最后的合奏中获得更多的可变性，我们还加入了SENet154[11]。

    3. 数据扩充

       >  数据扩充。在将图像传送到网络之前，我们进行了大量的数据扩充。我们使用随机亮度和对比度变化、随机翻转、随机旋转、随机缩放（使用适当的填充/裁剪）和随机剪切。此外，我们使用了一个孔的切口[7]，孔尺寸为16。我们尝试应用AutoAugment v0策略，但是没有观察到更好的性能。 
    
    4. 输入策略
    
       > 我们采用不同的输入策略进行训练，将图像从预处理后的原始大小转换成合适的输入大小。首先，我们遵循去年挑战赛中采用的相同规模的种植策略[9]。这里，我们从预处理后的图像中随机裁剪。其次，我们采用了imagnet训练中流行的随机调整大小策略[1]。在这里，当从预处理图像中获取裁剪时，图像被随机调整大小和缩放。
    
    5. 训练
    
       > 我们用Adam训练100个时代的所有模特。我们使用加权交叉熵损失函数，其中代表性不足的类在训练集中获得更高的基于权重的频率。每个类乘以系数ni=（N/ni）k，其中N是训练图像的总数，ni是类i中的图像数，k控制平衡严重性。我们发现k=1最有效。我们还尝试在没有性能改进的情况下使用具有相同平衡权重的焦点损失[12]。根据每种体系结构的GPU内存需求，采用批量大小和学习速率。我们每25个时代把学习减半。我们每10个阶段评估一次，并保存达到最佳平均灵敏度（best）的模型。另外，我们在经过100个阶段的训练（last）后保存最后一个模型。培训是在NVIDIA gtx1080ti（B0-B4）和Titan RTX（B5）图形卡上进行的。
  
- 基于卷积神经网络的面部痤疮分级评估方法

  1. 关于如何分级

     参照Hayashi等人分级标准，严重程度分为4类，仅与炎性皮损数目有关，

  2. 应用技术

     图像分类：ResNet残差网络

     物体检测：Fast-RCNN


# 人脸识别

## 流程

1. 人脸检测

   模板匹配；adaboost；深度学习：Cascade CNN、MTCNN

   1. 人脸对齐															

3. 人脸特征表示

   特征点识别。接受的输入是标准化的人脸图像，通过特征建模得到向量化的人脸特征，最后通过分类器判别得到识别的结果。

# 李宏毅机器学习

![image-20210117112454225](C:\Users\GY\AppData\Roaming\Typora\typora-user-images\image-20210117112454225.png)

**寻找函数，将输入转化为输出**

三类：regression、classification、generation

- 回归

  回归方法是一种对数值型连续随机变量进行预测和建模的监督学习算法。

  方法：线性回归、回归树（随机森林、梯度提升树）、深度学习、

- 分类

  分类方法是一种对离散型随机变量建模或预测的监督学习算法。使用案例包括邮件过滤、金融欺诈和预测雇员异动等输出为类别的任务。

  方法：逻辑回归（通过逻辑函数将预测映射到0、1）、决策树CART、深度学习、支持向量机svm、朴素贝叶斯NB

- 聚类

  聚类是一种无监督学习任务，该算法基于数据的内部结构寻找观察样本的自然族群（即集群）。使用案例包括细分客户、新闻聚类、文章推荐等。

  因为聚类是一种无监督学习（即数据没有标注），并且通常使用数据可视化评价结果。如果存在「正确的回答」（即在训练集中存在预标注的集群），那么分类算法可能更加合适。

  方法：K均值聚类、Affinity Propagation 聚类

[上述三类来源](https://www.zhihu.com/search?type=content&q=%E5%9B%9E%E5%BD%92%E3%80%81%E5%88%86%E7%B1%BB%E3%80%81%E8%81%9A%E7%B1%BB)

方法：supervised learing 寻找loss最小的函数、reinforcement learning、unsupervised learing

## 回归

- 步骤
  - 定义一个model即function set
  - 定义一个goodness of function损失函数去评估该function的好坏
  - 找一个最好的function

## 深度学习

[详细笔记](https://sakura-gh.github.io/ML-notes/ML-notes-html/8_Deep-Learning.html)

### 步骤

- 定义一个神经网络（weight、bias、激励函数（非线性，增强模型表达能力））：

  - 神经元（图中小球）

    一个神经元是一个逻辑回归模型，一个神经元有x1、x2、...、xN个输入，每个输入xi对应一个wi即weight；每个神经元有一个b即bias；每个下式中关于z的函数叫sigmoid function（还可以是其它函数），在深度学习中叫激励函数
    $$
    f(x) = 1/(1 + e^{z}) = 1/(1 + e^{-(\sum ^{n} _{i=1} x_iw_i+b)})
    $$

  - 

  ![image-20210117163148148](C:\Users\GY\AppData\Roaming\Typora\typora-user-images\image-20210117163148148.png)

  ​															*图中每一排表示一个层，每个球表示一个神经元*

   输入层通过隐藏层，即一系列矩阵运算转化为输出层。若是分类问题则在通过在输出层后面加softmax，输出的y1...yM映射到0~1上变为z1....zM，每个z都在0~1上

- 定义一个神经网络的好坏 

  即output和target计算出loss，即y1...yM和我们给定的标签比较计算出loss

- 找到最好的神经网络，即找到最好的\theta（weight、bias）

  梯度下降：正向传播、反向传播

  工具：tensorflow、pytorch

### tips

先与training set比较，看是否有好的结果，然后调参，再与testing set比较，看是否过拟合，然后调参。

- 关于激励函数

  sigmoid function：最原始的激励函数

  ReLU function：使激励函数变为类似线性的，从而避免梯度下降时变化不明显

  Maxout function： 可x成relu相同功能并且可以完成更多功能。因为input不同，不同max不同，则每个weight与bias都可以训练到

- 梯度下降时的learning rate

  adagrad

  RMSProp

- training set越来越好但testing set的loss可能上升

  **Early Stopping**：提前结束training，用validation set代替testing set，当validation set上升时即时停止

  **Regularization**：regularization就是在原来的loss function上额外增加几个term，比如我们要minimize的loss function原先应该是square error或cross entropy，那在做Regularization的时候，就在后面加一个Regularization的term

  **dropout**：training时，每次更新参数，对每个神经抽样，每个神经有一定几率被丢掉，若被丢掉则相连 的weight也被丢掉

### why deep

深层网络 ---》 [模组化](https://sakura-gh.github.io/ML-notes/ML-notes-html/14_Why-Deep.html) 

一层？多层？ 类似于逻辑电路，多层使逻辑门得到更多使用，更有效率。虽然一层即可达到任务，但是效率低下。

多层可以将输入进行多次映射，即可对复杂的问题进行分类。	

### CNN

影像识别

- 原因

  一个neural network也可识别。我们期待每个neuron是一个基本的分类器，然而input是图片时，输入的vector会特别长，而隐藏层的参数会更加多，我们希望简化网络的框架。即依靠先验知识对一些实际用不到的参数给过滤。

  - property1

    Some patterns are much smaller than the whole image

  - property2

    The same patterns appear in different regions

  - property3

    Subsampling the pixels will not change the object

    即去掉偶数行奇数列

- 步骤

  ​	convolution -》 maxpooling -》..... -》Flatten -》 Fully Connected Feedforward network

  - 卷积convolution

    - padding

      填充，即在卷积输入的图片周围填充一圈

      正常卷积尺寸变化： n * n    卷积   f * f  ----》 (n - f + 1)   * (n - f + 1)

      padding： (n + 2p) * (n + 2p)    卷积   f * f  ----》 (n + 2p - f + 1)   * (n  + 2p  - f + 1)  若p = (f-1)/2  则卷积后尺寸和卷积前相同

    - stride 步幅

      padding + stride： (n + 2p) * (n + 2p)    卷积   f * f   步幅s----》 (n + 2p - f)/s + 1   * (n  + 2p  - f)/s + 1  

    - 多通道卷积

      **假定用 N 表示filter的数量，那么每一个filter会生成一个 2 维的 feature(或matrix),N 个filter就生成 N 个 feature,N 个 feature 组成了卷积后的 featuremap，而 N 就是 featuremap 的通道数。**

      **filter的通道数要和卷积输入的通道数相同，若filter为立方体，那么卷积的结果也是立方体（方法就是每个通道分别卷积，结果叠起来就是立方体），在将该立方体各个通道对应坐标的值相加就生成了feature，相当于将多维的 Result 压缩成了 2 维的 feature（也可叫matrix，即矩阵）**

    - 意义

      用一打k * k的filter卷积图像中k*k的像素块得到的值，实质上就是该像素块作为神经网络输入，然后通过一层计算得到的输出，也就相当于过滤了一些用不到的参数。
      - filter为小范围：考虑到property1

      - 对图像用同一个filter卷积运算：考虑到property2

      用n个3 * 3 filter卷积1张6 * 6黑白图片 --> n个4 * 4的matrix（它们也叫做feature map 特征映射） 

    - 举例

      **【若为RGB图像，则一个filter为一个3 * 3 * 3立方体，卷积时每个3 * 3 * 3（长 * 宽 * 三通道）的filter和 4 * 4 * 3（长 * 宽 * 三通道）图像卷积，或者说看成3（通道数）个1  * （3 * 3）的filter 分别卷积 3（通道数）张 1 *（4 * 4）的图片，这时候每个卷积都为单通道（分别为R,G,B通道），卷积出3张再压缩为一张matrix 】**

      **【注意不论几次的convolution，每次convolution后都是得到filter个matrix】**  

      1. 25 * (3 * 3) filter 卷积  1 * (28 * 28) --> 25 * (26 * 26) feature map	

      2. 2 max pooling   --> 25 * (13 * 13)

      3. 50 * (3 * 3) filter 卷积 25 * (13 * 13)  --> 50 * (11 * 11) feature map   注意这里的filter不是一个3 * 3 的matrix，而是一个 25 * 3 * 3的立方体，因为原图像每个像素有25个通道，相应的feature map实际上也不是(11 * 11)的matrix而是 50 * 11 * 11

      4. 2 * 2 max pooling

      【filter也是经过不断训练得到的，训练时使参数不变filter变，梯度下降得到filter】

  - 池化max pooling

    把得到的4 * 4matrix分为四个组，每个组用取平均值或者取最大值方法化为一个2 * 2的matrix

    同卷积，maxpooling取得区域相当于filter，也有步幅

  - Flatten + Fully..

    展开得到的matrix，然后放入全连接神经网络中得到输出

 相关资料：https://www.zhihu.com/question/49376084

多通道卷积：https://blog.csdn.net/briblue/article/details/83063170

### RESNET

- 原理

  增加shortcut，即例如layer1的输出直接连到layer2输出层的激励函数之前直接相加

- 意义

  解决梯度爆炸

# Labelme

## anaconda安装与使用

1. anaconda安装

2. ```cmd
   conda create --name=labelme python=3.6	#conda开一个python环境并命名为labelme
   activate labelme	#进入该环境
   pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pyqt5	#清华源下pyqt5
   pip install labelme		#安装lableme
   ```

- 注意网络问题可能导致报错，可切换网络或者使用别的源

- 关于anaconda的[入门解释](https://www.jianshu.com/p/eaee1fadc1e9) 

  > 列举该环境下所有包	conda list 
  >
  > 复制备份环境	conda create -n study --clone base
  >
  > 导出环境的yml文件	conda env export > environment.yml
  >
  > 根据yml文件新建一个环境	conda env create -f environment.yml

- Python_medical

  > python 3.7.9
  >
  > pytorch      1.6.0           py3.7_cuda10.1.243_cudnn7.6.3_0    pytorch

## 使用

```cmd
#anaconda的prompt下
activate labelme	#进入python环境
labelme		#打开labelme
```

### 标注

opendir打开文件夹进行标注。

- 分割数据集

  标注完成后save生成相应jason文件，包含相应label和标注信息

  

### 批量化操作

```
https://blog.csdn.net/yql_617540298/article/details/81110685
```

- ```python
  AttributeError: module 'labelme.utils' has no attribute 'draw_label'
  ```

  https://blog.csdn.net/qq_34997364/article/details/105152927

# 数据集处理

目的 ：根据图片与相应json生成裁剪的痤疮图片

> patch可以按照一个固定的比例适当取的大一点，以包含更多的信息
>
> 我现在的命名方式是  “ID _ 痤疮级别 _ 视角”patch可以按照“ID _ 痤疮级别 _ 视角 _ 痤疮类型_编号”来命名
>
> 粉刺（黑头/白头） head	
> 丘疹 papule
> 脓疱 pustule
> 结节 nodule
> 囊肿 cyst
> 瘢痕 scar
>
> 痤疮类型命名，最后命名成这样 0001_4_front_nodule_1.png

每种痤疮类型编号从1开始

## 一些函数

- os.path.join()函数：

  连接两个或更多的路径名组件

  1. 如果各组件名首字母不包含'/'，则函数会自动加上
  2. 如果有一个组件是一个绝对路径，则在它之前的所有组件均会被舍弃

- os.walk(dir_path)函数：

  返回

  1. dirpath(目录路径，它是一个string类型)
  2. dirname(子目录名，它是一个列表，因为在一个目录路径下会有很多很多子目录)
  3. filename(文件名，它也是一个列表，因为同一目录下一班有多个文件

- shutil.copy

  shutil.copyfile(src, dst)：复制文件内容（不包含元数据）从src到dst。

   DST必须是完整的目标文件名;

  shutil.move剪切

- zip()

  可同时在for中循环两个变量

- for a,b,c in ..

  for a, b, c in rs 定义了a,b,c三个参数

  一般这是由于rs是个元组，rs=[(1,2,3),(4,5,6),(7,8,9)]，那么a,b,c返回1,2,3、4,5,6、7,8,9
  
- 提取正脸以及json文件

  ```python
  def copy_file():
      name_list = []
      dir = "E:\image-json\image-json/"
      face_list = os.listdir(dir)
      for json_file in face_list:
          file_name = json_file.split(".")[0]
          file_last = json_file.split(".")[1]
          if file_last == "json":
              if file_name.split("_")[-1] == "front":
                  name_list.append(file_name)
                  path = shutil.copy(dir + json_file, "E:/北航/毕设/face_detect/faces_front_json/" + json_file)
                  print(path)
      for face_file in face_list:
          file_name = face_file.split(".")[0]
          file_last = face_file.split(".")[1]
          if file_last == "jpg":
              if file_name.split("_")[-1] == "front":
                  if file_name in name_list:
                      path = shutil.copy(dir + face_file, "E:/北航/毕设/face_detect/faces_front/" + face_file)
                      print(path)
  ```

  

## 一些问题

- is和==

  is 判断两个变量是否是引用同一个内存地址。

  == 判断两个变量是否相等。

- 若相关文件缺失的错误处理

  先统计所有json文件，然后统计jpg文件，仅当jpg文件有相应json文件时才放入list

- 如何自动在相应目录下创建新的文件夹

  os.mkdir

- 注意要保存为png

# face-landmark人脸识别+对齐

## 安装

dlib 失败

Cmake 成功

Boost 成功

scikit-image 成功（似乎没啥用）

dlib最后安装方法：

[下载链接](https://pypi.org/simple/dlib/)

下载.whl文件，cmd下进入下载文件目录，`pip install dlib-19.8.1-cp36-cp36m-win_amd64.whl`下载成功

## 使用

```python

import dlib
import cv2
 
# 与人脸检测相同，使用dlib自带的frontal_face_detector作为人脸检测器
detector = dlib.get_frontal_face_detector()
 
# 使用官方提供的模型构建特征提取器
predictor = dlib.shape_predictor('E:data/shape_predictor_68_face_landmarks.dat/shape_predictor_68_face_landmarks.dat')
# cv2读取图片
img = cv2.imread("D:/2.0/progect/my_faces0/103.jpg")
 
# 与人脸检测程序相同,使用detector进行人脸检测 dets为返回的结果
dets = detector(img, 1)
# 使用enumerate 函数遍历序列中的元素以及它们的下标
# 下标k即为人脸序号
# left：人脸左边距离图片左边界的距离 ；right：人脸右边距离图片左边界的距离
# top：人脸上边距离图片上边界的距离 ；bottom：人脸下边距离图片上边界的距离
for k, d in enumerate(dets):
    print("dets{}".format(d))
    print("Detection {}: Left: {} Top: {} Right: {} Bottom: {}".format(
        k, d.left(), d.top(), d.right(), d.bottom()))
 
    # 使用predictor进行人脸关键点识别 shape为返回的结果
    shape = predictor(img, d)
    # 获取第一个和第二个点的坐标（相对于图片而不是框出来的人脸）
    print("Part 0: {}, Part 1: {} ...".format(shape.part(0), shape.part(1)))
 
    # 绘制特征点
    for index, pt in enumerate(shape.parts()):
        print('Part {}: {}'.format(index, pt))
        pt_pos = (pt.x, pt.y)
        cv2.circle(img, pt_pos, 1, (255, 0, 0), 2)
        #利用cv2.putText输出1-68
        font = cv2.FONT_HERSHEY_SIMPLEX
        cv2.putText(img, str(index+1),pt_pos,font, 0.3, (0, 0, 255), 1, cv2.LINE_AA)
 
cv2.imshow('img', img)
k = cv2.waitKey()
cv2.destroyAllWindows()
```

dets：人脸检测，检测屏幕中每张人脸，返回一个列表，列表中每个元素d表示人脸相对于图像的位置。

predictor(d)，对每张人脸进行关键点识别

**其中所有的距离以像素为单位  ------------  》 而json文件中的标注距离也是以像素为单位的**

## FaceTools-master 模型

注意：路径中不支持中文；环境为python2.0，因此需要修改

1. 人脸检测

   一张图片给出两个坐标，即方框左上角和右下角

2. 关键点检测

   原模型使用五个点检测，即左眼、右眼、鼻尖、左嘴角、右嘴角

3. 对齐

   利用该5点进行仿射变换使人脸归一化

仿射变换

问：人脸对齐对痤疮检测的作用？似乎...仿射变换对痤疮分类检测没啥帮助，直接怼yolo吧

## aam active appearance model

# cuda + tensorflow

## 安装

[与windows驱动对应版本](https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cuda-major-component-versions)

[CUDA、显卡驱动和Tensorflow、Pytorch版本之间的对应关系](https://blog.csdn.net/IT_xiao_bai/article/details/88342921?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-2.control&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-2.control)

[cuda和显卡对应](https://blog.csdn.net/weixin_43522055/article/details/99595641)

[keras教程](https://zhuanlan.zhihu.com/p/95065951)

[cuda cudnn安装教程](https://blog.csdn.net/sinat_23619409/article/details/84202651)**注意版本对应！！！**

cuda9.1

cudnn 7.1 

路径会自动配置，注意安装后重启

tensorflow教程在keras教程中，使用.whl安装

keras: pip3 install keras==2.1.4

# pytorch分类模型

## 安装

pip install torch===1.1.0 torchvision===0.3.0 -f https://download.pytorch.org/whl/torch_stable.html（注意版本对应）

conda install pytorch==1.6.0 torchvision==0.7.0 cudatoolkit=10.1 -c pytorch

也可进入相应下载网址，用迅雷下载，然后进入相应目录pip install  .whl

## 相关知识

[图像灰度、通道](https://www.cnblogs.com/heartchord/p/5903011.html)

- 灰度

  灰度也可认为是亮度，简单的说就是色彩的深浅程度。

  灰度值即各个通道数值

- 通道

  图像的颜色模式决定了图像颜色通道的数目。

  如RGB图4个通道：黑到白、红、绿、蓝三个通道

  灰度图一个通道：表现为黑色到白色256色阶变化

- 一些名词

  1. batchsize：每批数据量的大小。DL通常用SGD的优化算法进行训练，也就是一次（1 个iteration）一起训练batchsize个样本，计算它们的平均损失函数值，来更新参数。

  2. iteration：1个iteration即迭代一次，也就是用batchsize个样本训练一次。

  3. epoch：1个epoch指用训练集中的全部样本训练一次，此时相当于batchsize 等于训练集的样本数。

  4. learing rate：将输出误差反向传播给网络参数，以此来拟合样本的输出。本质上是最优化的一个过程，逐步趋向于最优解。但是每一次更新参数利用多少误差，就需要通过一个参数来控制，这个参数就是学习率（Learning rate）,也称为步长

  5. loss：损失函数，衡量我们对结果的不满意程度

  6. backbone：骨干网络 vgg、resnet...

  7. 测试集、训练集、验证集

     **测试集**，是用来测试已经训练好的模型的泛化能力。

     **训练集**是用来训练模型或确定模型参数的，如ANN中权值，CNN中的权值等

     **验证集**是用来做模型结构选择，确定模型中的一些超参数，比如正则项系数，CNN各个隐层神经元的个数等；
     
     > `training dataset`和`validation dataset`都是在训练的时候起作用。
     >  而因为`validation`的数据集和`training`没有交集，所以这部分数据对最终训练出的模型没有贡献。
     > `validation`的主要作用是来验证是否过拟合、以及用来调节训练参数等。
     
     > 验证集用在训练过程中，一般训练时，几个epoch结束跑一次验证集看效果，发现模型或者参数问题，例如模型在验证集发散，maP不增长，这时可及时终止训练，重新调整参数；另一个好处是验证模型泛化能力，如果验证集效果比训练集差很多需要考虑是否过拟合
     >
     > 总结：模型的参数包括普通参数和超参数（与模型设计和训练有关的一些参数），利用bp只能训练普通参数，而无法“训练”模型的超参数，因此，我们设置了验证集，通过验证集的效果进行反馈，根据效果看是否需要终止当前的模型训练，更改超参之后再训练，最终得到最优的模型！
     >
     > 注意：
     >
     > 1）针对超参的选择我们是根据验证集上的效果来进行调整的，因此验证集可以看做参与到“人工调参”的训练过程；
     > 2）注意训练集、验证集和测试集应该服从同一数据分布，这样我们才能进行玄学调参；
     > 3）测试集是可以没有的，但验证集是必须有的，如果验证集具有足够的泛化代表性，是可以不需要设置测试集的；注意测试集的存在只是为了验证我们在训练集和验证集上进行模型的超参和参数训练后，验证我们得到的模型是否具有泛化性能！
     
     注意一开始就把训练集和测试集分开，训练集中可以再划分出验证集

- dir()  help()活用，每个函数可获取其使用方法

- 读取数据

  - dataset

    获取数据及其label

    继承dataset类，重写init和getitem方法（需要使用os.listdir()）

    > `torch.utils.data.Dataset`是一个抽象类，用户想要加载自定义的数据只需要继承这个类，并且覆写其中的两个方法即可：
    >
    > 1. `__len__`:实现`len(dataset)`返回整个数据集的大小。
    >
    > 2. `__getitem__`用来获取一些索引的数据，使dataset[i]返回数据集中第i个样本。
    >
    >    getitem可以返回一个字典{‘image':img, 'label':label}，也可以返回其它方式
    >
    > 3. 不覆写这两个方法会直接返回错误。

  - dataloader

    > Dataloader 就是一个迭代器，最基本的使用就是传入一个 Dataset 对象，它就会根据参数 batch_size 的值生成一个 batch 的数据
    >
    > ```python
    > dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2)
    > for idx, batch_samples in enumerate(dataloader):
    >     imgs, labels = batch_samples['image'], batch_samples['label']
    > ```

- 图像转换

  - tensorboader

## 学长代码

- data文件夹

  train_image：分类图片

  train.xls：每张分类图片对应标签

- datautils文件夹

  有关数据处理，生成训练集、评估集、测试集，train.py以及test.py调用其get_dataloaders和get_test_dataloaders获取loader

  - Dataset.py

    获取数据集

    - python函数

      ```python
      reader = csv.reader(f) 此时reader返回的值是csv文件中每行的列表，将每行读取的值作为列表返回
      ```

    - `def __init__`

      split:'train'	'val'   训练与评估

      dataroot: 

      K:K_fold -》 数据集分为5块，4块用于测试

      transforms:train_transforms	valid_transforms

      folder: 'train_images' 当split为'test'则为'test_images'

      img_path：dataroot\test_images	dataroot\train_images	

      transform：图形变换，将图像变换为固定大小，

      ./datautils/data.json 就为训练集和评估集来源，其中K取值为01234，K作为评估集，其余作为训练集，data.json以726张图片为一组分为5组用于训练与评估。json文件中的图片命名对应于data\train_images中的图片名

    - `def __len__:`

      ```python
      return len(self.samples)
      ```

      获取数据集长度

    - `def __getitem__`

      folder为test_image，则从data\test_image中取图片和标签；

      否则从data\train_image中取图片和标签

      返回相应index的图片和标签

    - `def get_dataloaders`

      返回train_loader, val_loader，即训练loader和评估loader。其中所有图片都经过transfer为固定大小
      
    - 问
    
      有啥用？imread读入的是BGR格式，需要转换为RGB格式
    
      ```python
      image = cv2.imread(img_path, cv2.IMREAD_IGNORE_ORIENTATION + cv2.IMREAD_ANYCOLOR).astype(np.float32)
      if len(image.shape) == 3:
          image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
      image = np.float32(image)  # (H, W, C), value in (0, 255)
      ```
    
    重写Dataset.py，`__init__`、`def __len__`、`def __getitem__`都无所谓传入什么，只要`def get_dataloaders`传入一项args即可，因为我并不准备作K组交叉验证
    
    获取一个总的dataset，然后使用[SubsetRandomSampler](https://www.cnblogs.com/marsggbo/p/10496696.html)按照一定比例获取训练集、验证集、测试集，
    
  - transforms.py

    数据增强

    - get_transforms()

      颜色抖动ColorJitter，改变图像的属性：亮度（brightness）、对比度（contrast）、饱和度（saturation）和色调（hue)

      RandomVerticalFlip(),RandomHorizontalFlip()：随机水平垂直翻转

      RandomGaussianBlur()：通常用它来减少 [图像噪声](http://baike.baidu.com/view/944141.htm)以及降低细节层次

      Normalize()：提供一个所有通道的均值（mean） 和方差（std），会将原始数据进行归一化

      

- 根目录

  - train.py

    - `train_epoch`  模型训练

      epoch：训练集训练次数

      model：图像分类模型

      dataloader：训练集

      optimizer：如梯度下降算法，BGD、SGD、MBGD

      print_freq：打印频率，每多少组数据打印一次

      logger：源于[logging](https://www.cnblogs.com/xianyulouie/p/11041777.html)，方便记录训练信息

      writer：用于实现训练过程可视化，如各项指标kap...

      Tqdm：可扩展的Python进度条，可以在 Python 长循环中添加一个进度提示信息
    
      (N, H, W, C) ~ (n, c, h, W)：将图像格式转换
    
      AvgMeter：统计各项指标的平均值	
    
      - 流程
    
        对每张img和label，`pred, loss = model(img, label)`得出loss，`optimizer.zero_grad()`使梯度置零，`scaled_loss.backward()`反向传播？，`optimizer.step()`更新参数
    
    - `validate_epoch` 模型评估，计算kap值表
    
      ​	`with torch.no_grad()：`：包含语句内将不会计算梯度
    
       - 流程
    
         与train不同，不更新参数，只记录acc等指标到list中，因此`pred, loss = model(img)`后就不更新参数，仅记录
    
    - `train`
    
      model：`model = build_model(args)`，最终来源undefined_model2.py的 ClassificationModel类
    
      criterion：损失函数
  
- models

  - unified_model2.py

    关于卷积网络模型的定义

    该模型由backbones中的resnet模型和nn.Sequential合并构成一个联合的模型，forward函数一次通过他们两个
    
    - self.criterion
    
      损失函数，
    
    - def forward()
    
      正向传播，model(img,label)也是调用该函数
    
      ```python
      def forward(self, img, label=None):
          #通过backbone
          img_feature, lay = self.backbone(img)
          #通过pred_header  得到一个num_labers维的向量pred
          pred = self.pred_header(img_feature)
          loss = 0.
          if label is not None:
              #计算loss
              loss = self.criterion(pred, label)
          #选取pred中取值最大的，即分类评分最高的为最后的分类
      	pred = pred.max(1)[1]
          return pred, loss
      ```
    
      
  
- utils

  - ld_sv.py

    - def save_checkpoint(state, is_best, fpath='checkpoint.pth.tar'):

      fpath一般传入log_path/ckp_ep_kap.pth.tar (log_path为.resnest50)

- test.py

  - tensor()取里面的值用.item()（注意得仅仅有一个值的时候）

  - tensor()的维度

    三维基于二维增加了一维，即（2，2，3）是包含了2个2行三列的矩阵；第一个数字即指包含几个二维矩阵
  
    四维基于三维增加了一维，（2，2，2，3）即包含了2个三维的矩阵。
  
    若是n维，以此类推，基于上一维增加一维计算。
  
  - tensor()索引操作
  
    ```python
        import torch as t  
          
        a = t.randn(3,4)  
        '''''tensor([[ 0.1986,  0.1809,  1.4662,  0.6693], 
                [-0.8837, -0.0196, -1.0380,  0.2927], 
                [-1.1032, -0.2637, -1.4972,  1.8135]])'''  
        print(a[0])         #第0行  
        '''''tensor([0.1986, 0.1809, 1.4662, 0.6693])'''  
        print(a[:,0])       #第0列  
        '''''tensor([ 0.1986, -0.8837, -1.1032])'''  
        print(a[0][2])      #第0行第2个元素，等价于a[0,2]  
        '''''tensor(1.4662)'''  
        print(a[0][-1])     #第0行最后一个元素  
        '''''tensor(0.6693)'''  
        print(a[:2,0:2])    #前两行，第0,1列  
        '''''tensor([[ 0.1986,  0.1809], 
                [-0.8837, -0.0196]])'''  
          
        print(a[0:1,:2])    #第0行，前两列  
        '''''tensor([[0.1986, 0.1809]])'''  
        print(a[0,:2])      #注意两者的区别，形状不同  
        '''''tensor([0.1986, 0.1809])'''  
    ```
  
    即对应tensor维度取值，取出来还是一个tensor
  
  `from sklearn.metrics import confusion_matrix`可获得混淆矩阵
  
  

## 细节

```python
def get_transforms():

    train_transforms = [
        ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0),
        Resize(size=(492, 492)),
        RandomVerticalFlip(),
        RandomHorizontalFlip(),
        RandomGaussianBlur(),
        # ToTensor(),
        Normalize(mean=mean, std=std)
    ]

    valid_transforms = [
        Resize(size=(492, 492)),
        # ToTensor(),
        Normalize(mean=mean, std=std)
    ]

    return Compose(train_transforms), Compose(valid_transforms)
```

1. 标注边框数量分布：cyst：192	head：4197	nodule：730	papule：9488	pustule：2723	scar：2339
2. 输入图像resize到492 * 492， 随机水平垂直翻转，高斯滤波器进行平滑处理，图像归一化处理
3. cnn模型：resnest50
4. 数据集测试集：4:1比例  从所有图像中划分按比例划分数据集和测试集
5. [[ 11   0   2   3   1   0]
    [  0 318   0  22   3   5]
    [  6   0  23  42   0   2]
    [  1  11  10 654   4   1]
    [  0   7   0  33 182   1]
    [  0   5   0   9   0 145]]
   cyst_precision: 0.6111111111111112 recall: 0.6470588235294118
   head_precision: 0.9325513196480938 recall: 0.9137931034482759
   nodule_precision: 0.6571428571428571 recall: 0.3150684931506849
   papule_precision: 0.8571428571428571 recall: 0.960352422907489
   pustule_precision: 0.9578947368421052 recall: 0.8161434977578476
   scar_precision: 0.9415584415584416 recall: 0.9119496855345912

- ROC曲线

  > (1)若一个实例是正类并且被预测为正类，即为真正类(True Postive TP)
  >
  > (2)若一个实例是正类，但是被预测成为负类，即为假负类(False Negative FN)
  >
  > (3)若一个实例是负类，但是被预测成为正类，即为假正类(False Postive FP)
  >
  > (4)若一个实例是负类，但是被预测成为负类，即为真负类(True Negative TN)
  >
  > **TP**:正确的肯定数目    True Postive
  >
  > **FN**:漏报，没有找到正确匹配的数目  False Negative
  >
  > **FP**:误报，没有的匹配不正确 False Postive
  >
  > **TN**:正确拒绝的非匹配数目 True Negative
  >
  > 
  >
  > (1)真正类率(True Postive Rate)TPR: **TP/(TP+FN)**,代表分类器预测的**正类中**实际正实例占所有正实例的比例。Sensitivity
  >
  > (2)负正类率(False Postive Rate)FPR: **FP/(FP+TN)**，代表分类器预测的**正类中**实际负实例占所有负实例的比例。1-Specificity
  >
  > (3)真负类率(True Negative Rate)TNR: **TN/(FP+TN)**,代表分类器预测的**负类中**实际负实例占所有负实例的比例，**TNR=1-FPR**。Specificity
  >
  > 
  >
  > **roc曲线**：接收者操作特征(receiveroperating characteristic),roc曲线上每个点反映着对同一信号刺激的感受性。
  >
  > **横轴**：负正类率(false postive rate FPR)特异度，划分实例中所有负例占所有负例的比例；(1-Specificity)
  >
  > **纵轴**：真正类率(true postive rate TPR)灵敏度，Sensitivity(正类覆盖率)
  >
  > 横轴从左到右，表示阈值不断增大（0 ~ 1），对于二分类问题，超过阈值判正，低于阈值判负
  >
  > https://blog.csdn.net/ye1215172385/article/details/79448575



# pytorch yolov3 目标检测模型

## YOLO

- yolov1

  **grid cell**：图像划分成S×S的格子

  **bounding box**：**x,y,w,h,confidence**（Pr(Object) * IOU）

  **ground truth**：给定标注框

  **Pr(Object)**：如果ground truth落在这个grid cell里，那么Pr（Object）就取1，否则就是0

  **iou**：bounding box与实际的groud truth之间的交并比

  损失函数：bounding box损失 + confidence损失 + classes损失  

  > 作者在YOLO算法中把物体检测（object detection）问题处理成回归问题，用一个卷积神经网络结构就可以从输入图像直接预测bounding box和类别概率。
  >
  > 算法首先把输入图像划分成S×S的格子**grid cell**，然后对每个格子都预测B个**bounding boxes**，每个bounding box都包含5个预测值：**x,y,w,h**和**confidence**。
  > x，y就是bounding box的中心坐标，与grid cell对齐（即相对于当前grid cell的偏移值），使得范围变成0到1；
  > w，h进行归一化（分别除以图像的w和h，这样最后的w和h就在0到1范围）。
  > confidence代表了所预测的box中含有object的置信度和这个box预测的有多准两重信息：
  > 换句话说，如果ground truth落在这个grid cell里，那么Pr（Object）就取1，否则就是0，**IOU**就是bounding box与实际的groud truth之间的交并比。所以confidence就是这两者的乘积。
  >
  > 在yolov1中作者将一幅图片分成7x7个网格(grid cell)，由网络的最后一层输出7×7×30的tensor，也就是说每个格子输出1×1×30的tensor。30里面包括了2个bound ing box的x，y，w，h，confidengce以及针对格子而言的20个类别概率，输出就是 7x7x(5x2 + 20) 。
  > (通用公式： SxS个网格，每个网格要预测B个bounding box还要预测C个categories，输出就是S x S x (5×B+C)的一个tensor。 注意：class信息是针对每个网格的，confidence信息是针对每个bounding box的
  >
  > 

- yolov2

  1. BN
  
  2. High Resolution Classifier  高分辨率
  
     输入图片分辨率更大
  
  3. Convolutional With Anchor Boxes  使用先验框
  
  4. *Dimension* *Clusters*(使用聚类算法提取anchor boxes的宽高)
  
  5. Direct location prediction
  
  6. Fine-Grained Features（使用passthrough layer 融合大小不同的特征图）
  
  7. Multi-Scale Training  多尺度训练
  
     没迭代10个batch，就将网络输入尺寸进行随机的选择（为32的整数倍，32为缩放因子）
  
  骨干网络：darknet
  
- yolov3

  [从零开始学习yolov3](https://www.cnblogs.com/pprp/p/12199731.html)
  
  骨干网络：darknet-53	
  
  - 模型结构
  
    有3个预测特征层，每个预测特征层的每个cell有3个bounding box，所以每个预测特征层维度为 N * N * (3 * (4+1+80))  以coco为例（3：3个box，4：box的坐标偏移信息，1：confidence，80：80个类），3（预测特征层） * 3（3个bounding box） 就为anchor个数
  
  - 目标边界框的预测
  
    
    $$
    b_x = \sigma(t_x) + c_x \\
    b_y = \sigma(t_y) + c_y \\
    b_w = p_w e ^{t_w}	\\
    b_h = p_h e ^{t_h}	\\
    c_x \  c_y为cell左上角坐标 \\
    t_x \ t_y为anchor偏移	\\
    \sigma为sigmod函数，防止anchor偏移到别的cell
    $$
    每个cell有三个box，box
  
  - 损失计算
  
    我们的预测值形象化为每个cell都有9个bounding box，而数值化即为N * N * (3 * (4+1+80))，即用这个向量来计算损失
  
    - 正负样本选择
  
      **正样本的选择：**
      首先计算目标中心点落在哪个cell上，然后计算这个cell的9个先验框（anchor）和目标真实位置的IOU值（直接计算，不考虑二者的中心位置），取IOU值最大的先验框和目标匹配。于是，找到的 该grid中的 该anchor 负责预测这个目标，其余的网格、anchor都不负责。
  
      **负样本的选择：**
       计算各个先验框和所有的目标ground truth之间的IOU，如果某先验框和图像中所有物体最大的IOU都小于阈值（一般0.5），那么就认为该先验框不含目标，记作负样本，其置信度应当为0
  
      **不参与计算部分**
       这部分虽然不负责预测对象，但IOU较大，可以认为包含了目标的一部分，不可简单当作负样本，所以这部分不参与误差计算。
  
    - loss计算
  
      正样本：置信损失+分类损失+定位损失
  
      负样本：置信损失
  
      不参与计算：无
  
- yolov3-spp

  - mosaic图像增强

    图像旋转拼接...，增加数据多样性增加目标个数

  - spp结构

    网络结构加入spp结构融合不同尺度特征

  - 损失函数

    引入IoU loss

    引入GIoU Loss

    引入DIoU Loss，CIoU Loss

  - focal loss

    关于one-stage模型（ssd，yolo），存在问题：正负样本不平衡，没匹配到的候选框远大于匹配到的

## 安装

> git clone https://github.com/ultralytics/yolov3.git
> conda create -n yolov3 python=3.8
> pip install -r requirements.txt
> 注意要使用pytorch-gpu则将requirements里的pytorch 和 torchvision注释
>
> conda install pytorch==1.6.0 torchvision==0.7.0 cudatoolkit=10.1 -c pytorch

## 数据集准备

- data文件夹

  - images/

    存放图片

  - jsons/

    图片的标注信息

  - ImageSets/

    test.txt train.txt trainval.txt val.txt

    每个txt文件中每一行表示一个图片的名称

    trainval.txt是train和val总计

  - labels/
  
    对应每张图片一个同名.txt文件夹，内容为该图片的label信息：类别（0，1，..）  +  矩形框中心x,y + 矩形框宽w，高h （x,y,w,h要化为相对大小）
  
  - test.txt、train.txt、val.txt

    data文件夹下，为不同图片集的路径，一行一个路径

  - lesion.xml
  
    关于数据集的配置信息：num_classes、names、test train val.txt的路径
  
- 交叉验证？

  > 如果有独立的测试集，那么将训练集分成S份，此时训练集和验证集是S-1:1
  > 如果没有独立的测试集，也就是只有一个txt的文件数据，那么不需要划分训练集和测试集，直接将这些数据分为S份，根据S-1:1进行S轮，选择损失函数评估最优的模型和参数（也就是没有测试集的事情了，就是利用训练集和验证集）
  >
  > 
  >
  > 问：“假设我们分了10组数据做交叉验证，10种组合可以迭代训练出10组模型参数，我们选择最好的一组模型参数作为最终结果就行了。” 十组中最好的那一组的模型参数作为最终的模型参数？但是这组参数只是在这组验证集上效果最好啊，并不能说明在整体训练集（十组）上效果好啊
  >
  > 回答： 你好，这是最简单的一种方式，实际在生产中，可能会还保留单独的一份没有参加交叉验证的独立测试集来分别验证10组模型的准确度，选择在独立测试集上 表现最好的模型参数。

## 预训练模型模型准备

可以到readme.md中找到下载网址

有yolov3.pt、yolov3-spp.pt、yolov3-tiny.pt   都是pytoch版本

需要修改 models\里面的配置信息如yolov3.yaml，将num_classes修改了

python train.py --data data/lesion.yml --weights yolov3.pt --device 2

## 训练过程

python  train.py --data data/lesion.yaml  --cfg yolov3-spp-GY.yaml  --weights runs/train/GY-yolov3-spp-changeAnchor/weights/best.pt  --device 1  --name GY-yolov3-changeAnchor-trainNankai --epochs 150

name：保存路径名字

data：数据集配置

cfg：模型配置

weight：预训练模型

device：gpu

- def train(...)

  - 关于验证集的参与

    `test_path = data_dict['val']`，因此验证集实际上参与了，而名字为test_path(怪怪的)

    每个epoch训练结束后会计算一次P,R,maP等值，用的就是验证集`results, maps, times = test.test(...)`

## 模型评估

runs/train/GY-yolov32/results：

左到右：epoch	gpu	box	obj	cls	total	targets	img_size	precision	recall	mAP@.5	mAP@.5:.95

box：

obj：

precision：

recall：

mAP@.5：即将IoU设为0.5时，计算每一类的所有图片的AP，然后所有类别求平均，即mAP

mAP@.5:.95：表示在不同IoU阈值（从0.5到0.95，步长0.05）（0.5、0.55、0.6、0.65、0.7、0.75、0.8、0.85、0.9、0.95）上的平均mAP

- test

  python test.py --weights runs/train/GY-yolov3-tiny-cut/weights/best.pt --data lesion.yaml --img 640 --device 0 --name yolo-tiny-cut  --single-cls --task test

  **注意--task test才是测试模型**
  
  `--conf-thres`
  
  `--iou-thres`
  
  `--augment`

## 实现过程

- 主干特征提取网络：darknet

  输入  416 * 416 3通道  ----》 13 * 13 1024通道 		不断进行下采样，压缩高和宽扩张通道数

- 特征金字塔




# shell + 远程服务器使用

- Shell 是一个用 C 语言编写的程序，它是用户使用 Linux 的桥梁。

- cmd运行shell

  只要安装git，在git下sh即可进入shell	

- 运行.sh文件

  shell下`sh  abc.sh`

- 登录服务器

  注意需要挂上校园网vpn，可以ping 一下 10.134.171.254

  > 进入cmd，输入sh进入shell
  >
  > ssh liusheng_2020@10.134.171.254
  >
  > 2020iripls
  >
  > 断开连接：logout

- pycharm连接服务器

  https://www.cnblogs.com/liangjiongyao/p/8794324.html

  ![image-20210203171835239](C:\Users\GY\AppData\Roaming\Typora\typora-user-images\image-20210203171835239.png)

  然后在tool->Browser Remote Host可以显示远程的文件内容

  upload和download可以上传到或从远程下载文件
  
- tmux

  > 1. 新建会话`tmux new -s my_session`。
  > 2. 在 Tmux 窗口运行所需的程序。
  > 3. 按下快捷键`Ctrl+b d`将会话分离。
  > 4. 下次使用时，重新连接到会话`tmux attach-session -t my_session`。
  
  > 翻页模式：先按 ctrl ＋ｂ，松开，然后再按 [
  >
  > 退出按q



# 问题

现在有6个文件夹，6个种类的痤疮图片

整个流程？模型的话ResNet够用？pytorch？访问实验室服务器？

cmd中登录sh，进入服务器

conda新建一个自己的环境；服务器下新建自己的文件夹，在该文件夹下跑.py文件

指定GPU 最多1 ~ 2 张卡

tmux 使进程挂着

gpustate   

多个指标来显示准确率 ： kap指标   数据集不平衡

需要修改：值表改进，getloader，unified_modelr2.ClassificationModel.num_laber ， amp 使得batch_size只能是2的n次方, train.py的parser.add_argument



{'cyst': 0, 'head': 1, 'nodule': 2, 'papule': 3, 'pustule': 4, 'scar': 5}

from datautils.Dataset2 import get_dataloaders
train_loader, valid_loader = get_dataloaders()





问题：

- run.sh中 CUDA_VISIBLE_DEVICES=1,2   python train.py --lr 1e-3 \ 为什么挂2张卡直接报错
- log是如何查看的
- 关于模型存储在哪里



最终目标：给一张图片，标出痤疮严重程度		痤疮检测---》痤疮分类---》严重程度

开题中最重要的：**痤疮检测**   可能有多个伪点，需要进行分类判断

创新点明显，做的人少

**文献需要按需阅读**

没有不确定性，已经收集完备数据集

皮肤镜

# 开题

> **计算机视觉 ---  皮肤**
>
> 皮肤是人体最大的器官，也是所有临床工作人员在看望患者时进行的第一个评估领域，因为它可以提供有关患者基本健康状况的众多见解。例如，可以通过检查皮肤来评估心脏功能，肝功能，免疫功能和身体伤害。此外，皮肤病主诉在基层医疗中也最为普遍[1]。皮肤图像是医疗保健中最容易捕获的医学图像形式，并且该域与其他标准计算机视觉数据集具有相同的质量，是标准计算机视觉任务与医学应用之间的天然桥梁。
>
> 
>
> 面向疾病预防的病变检查，包括有无病变、病理类型，是健康检查的基础任务。基于计算机的病变检测，是计算机视觉技术在智慧医疗中的重大体现，并且非常适合引入深度学习。在基于计算机的病变检测方法中，一般通过监督学习方法或经典图像处理技术（如过滤和数学形态学），计算并且提取身体部位或器官在健康状态下的特征工程。其中，基于监督学习的机器学习方法，它所使用的训练数据样本，需要专业医师提供全面的病理影像，并手工标注。特征工程计算过程产生的分类器，将特征向量映射到候选者来检测实际病变的概率。
>
> 温妮——P9	皮肤镜 + 除了皮肤镜图片，卷积神经网络在大类别皮肤病的普通数码照片中也具有良好的分类效果
>
> **作用意义**
>
> 得益于深度学习技术的快速发展，计算机视觉技术和应用得到了显著进步，并推动了各行业的智能化、信息化发展。由于医疗保健数据的敏感性和权威性，医疗卫生保健领域的深度学习，尤其是医学图像技术，发展速度非常慢。我们需要研究更稳定可靠的、普适的深度学习方法，以便有效地处理复杂的医疗影像数据。当然，随着现代医疗系统的发展和优化，如何系统地引入计算机视觉的最新成果，实现与多学科理论的交叉融合，提升和优化临床治疗水平，医务人员和理论技术人员之间的交流就显得越来越重要。这也是现代智慧医疗应该思索的问题。无论如何，医学图像处理技术作为提升现代医疗诊断和治疗水平的重要工具, 使实施风险低、创伤性小的手术方案成为可能，必将在医学信息研究领域发挥更大的作用。
>
> 温妮——P11痤疮造成患者自卑和社会隔离，不规范治疗会.....科学方法对患者及整个社会意义重大



> **相关研究**
>
> - [**ISIC 2018：**](http://challenge2018.isic-archive.com/)据美国癌症协会称，皮肤癌是最常见的癌症形式。尽管可以通过直接检查及早发现，但与良性病变的视觉相似性使这项工作变得困难。引入皮肤镜成像可以更好地可视化皮肤病变的关键细节，从而提高诊断准确性。该[国际皮肤成像协作（ISIC）](https://isdis.net/)已经组织了世界上最大的皮肤（23000图像）的皮肤镜图像资料库，以支持研究和市场细分，临床属性检测方法的开发和疾病分类。该数据集是用于2018年挑战赛的快照，其中包含10,000多个用于训练的图像和1500用于测试的图像。**另请参阅：** [挑战摘要](https://arxiv.org/abs/1902.03368)/[资料说明](https://arxiv.org/abs/1803.10417)
> - [**SD-198：**](https://drive.google.com/open?id=1YgnKz3hnzD3umEYHAgd29n2AwedV1Jmg)受198种疾病状态影响的6,584张消费者级皮肤照片。**另请参阅：** [论文参考](http://xiaoxiaosun.com/docs/2016-eccv-sd198.pdf)/[作者网站](http://xiaoxiaosun.com/)
> - [**Derm7pt：**](https://github.com/jeremykawahara/derm7pt)超过2,000张皮肤皮肤病的皮肤镜检查和临床图像，并附有7点检查清单标准和注明的疾病诊断信息。
>
> IBM 研究院一直致力于将新技术应用于皮肤图像分析中去，参与该项目的除了医学专家，IBM 的[机器学习](https://www.im2maker.com/tags/机器学习)、计算机视觉和云计算专家都投入了巨大的精力。在IBM的设想中，未来，医护人员只需利用皮肤镜拍摄一张病变患处的图片并上传到云端分析服务中心，不久后就能获得详细的检测报告。随后，医生只需详细分析报告中的数据，就能判断病人是否患了黑色素瘤。此外，除了检测皮肤癌，报告中的数据还涉及患者的皮下细胞结构，医生能及时判定患者是否还有罹患其他疾病的可能。
>
> 
>
> 温妮——P17
>
> 卷积网络前Chang、Malik基于SVM分类器和特征提取的痤疮分类方法
>
> Sheng 2017 VGG16识别痤疮的6种临床表现
>
> 现存痤疮严重程度分级标准为分级标准和计数标准，在计算视觉中对应图像分类与物体检测
>
> 分级标准：resnet直接输入图片进行分类，预测结果P27     灵敏度还不错，但中度和重度不准，因为样本较少
>
> 计数标准：Faster R-CNN学习标注出的训练集中的丘疹、脓包边框，对测试集中的检测丘疹、脓包进行检测，对各个候选框的预测分类结果排序，给定阈值进行筛选，获得丘疹、脓包数量，根据Hayashi分级方法进行分级		对重叠面积大的检测不准确P38
>
> 关于Faster R-CNN原理P23
>
> 

> **VISIA**
>
> VISIA皮肤检测仪是一种能对皮肤的病理学特征进行定量分析的仪器。Visia皮肤检测仪能对皮肤色斑、[毛孔](https://baike.baidu.com/item/毛孔/8622433)、[皱纹](https://baike.baidu.com/item/皱纹/8622545)、平整度、[卟啉](https://baike.baidu.com/item/卟啉/8434018)、紫外线斑和日光损伤定量评估。

> **从自拍照图像评估面部痤疮严重程度的计算机视觉应用程序**
>
> 痤疮，医学上称为寻常痤疮，发生在毛孔堵塞死皮细胞和石油。这会产生黑头、白头，随着炎症的恶化，还会产生红色的丘疹[9]。传统上，痤疮严重程度评估是由皮肤科医生在临床环境。由皮肤科医生进行处方治疗，或根据严重程度推荐非处方护肤品[7]。
>
> 因为这种疾病是如此的普遍，痤疮患者对痤疮严重程度的需求超过了皮肤科医生对痤疮严重程度的评估。据估计，痤疮患者必须等待平均超过32天的预约与他们的皮肤科医生[1]。这对痤疮患者来说是一个巨大的挑战和挫折，因为它延迟了对饮食、生活方式和皮肤的指导
>
> 护理产品。
>
> 为了填补这一空白，NSH与微软合作开发了一款用于痤疮评估的消费者移动应用程序。移动应用程序有两个主要功能：1）根据上传的自拍图像准确评估用户的痤疮严重程度；2）根据具体的严重程度推荐合适的治疗方案，考虑性别、年龄、皮肤类型等人口统计信息。YouTube 1上提供了移动应用程序的简短演示在13'50''。
>
> 虽然该应用程序并不打算取代皮肤科医生进行临床评估，但它允许NSH积极与客户接触，使皮肤科专业知识用户友好，并可供客户使用，并及时满足客户的需求。这个应用程序大大缩短了他们的产品与实时客户数据的反馈回路，从而使公司能够主动设计新的或改进现有的产品。
>
> 以往的生物医学图像分类工作主要集中在使用高分辨率或近景图像，而不是自拍图像。Esteva等人[4]使用129450张高分辨率聚焦图像训练CNN模型，将图像分为良性和恶性皮肤癌。Choi等人[3]以预训练的VGG-19模型为特征提取工具，建立了随机森林迁移学习模型，对10类视网膜图像进行分类。训练数据集包括397张近景和聚焦图像。
>
> 以前关于痤疮评估的工作都没有部署到真正的移动应用程序中，也没有与推荐的护肤品相一致。NSH发现需要一种工具，使消费者能够安全、准确地自我评估、治疗和跟踪愈合过程。
>
> 这项工作的主要贡献总结如下：
>
> （1） 我们描述了第一个用于痤疮评估的移动应用程序，它只使用手机自拍图像而不是临床高分辨率图像，在皮肤科医生的水平上具有准确性。
>
> （2） 针对CNN模型对小样本训练数据的空间敏感性问题，提出了一种新的人脸图像增强方法。它显著提高了模型在测试图像上的泛化能力。
>
> （3） 我们建立了一个真实世界的皮肤管理移动应用程序，促进整个治疗周期，包括皮肤科医生，用户和皮肤护理产品。
>
> 遵循共同的策略来解决有限的
>
> 数据，我们应用以下图像处理步骤来增强
>
> 训练图像集。这些步骤的目的是尽量减少背景噪音，过大或过小-曝光，低分辨率，并使训练模型的空间强健。结果证明，这可以提高泛化性能在测试图像上。本节的其余部分将介绍这条管道的各个步骤。
>
> 2.2.1从面部皮肤提取皮肤贴片。虽然痤疮可能发生在脸上的任何地方，痤疮的主要部位是前额，脸颊和下巴。在这一步中，我们提取了皮肤补丁从前额，双颊，和下巴使用OpenFace人脸识别库。事实证明，这是成功的基准数据集中的接近人类的精确度[2，6]。
>
> 我们提出了一种耦合的方法来提取两种情况下的皮肤贴片正面人脸图像和侧视图图像。面部标志模型是第一步。如果在第一步中没有检测到人脸，我们采用OpenCV单眼模型检测出目标的位置一只眼睛。OpenCV模型是一个使用一种基于Haar特征的级联分类器[10]。基于眼睛位置，我们推断出前额，脸颊和下巴的区域皮肤补丁。因此，我们从每张人脸图像中提取了2-4张皮肤面片取决于图像视角。整个痤疮都一样由皮肤科医生指定的严重性标签分配给所有患者来自同一图像的皮肤补丁。图2显示了来自landmark facial的皮肤贴片的结果检测。每个补丁都有一个名称来区分在皮肤补丁之间。
>
> 2.2.2滚动皮肤贴片。CNN模型是空间敏感的。它意味着如果测试图像上的某个特征出现在新位置在训练中从未见过同样的特征CNN模型无法识别它。这带来了重大挑战。首先，痤疮的严重程度并不重要不取决于痤疮病灶在哪里。相反，它主要是取决于每个痤疮病变的严重程度，以及有多少是。二是训练图像数量少，且青春痘多病变只出现在有限的部位。因此测试图像上痤疮病灶的位置很可能是与训练图像相比是新的。结果是CNN模型不能很好地推广到测试图像上。为了缓解这个问题，我们将每个皮肤贴片滚动一段时间
>
> 像素数。额头图像块从右到右滚动如图3所示。脸颊和下巴的图像补丁是滚动的从下到上。轧制步长如式（1）所示：
>
> 𝑟𝑜𝑙𝑙𝑙𝑠𝑖𝑧𝑒=国际。
>
> /
>
> 012 3 (1)
>
> 其中X表示滚动方向上的像素数。N是
>
> 滚动次数。
>
> 选择参数N进行滚压后的皮肤贴片
>
> 在所有5个班级中几乎平衡。轻微的图像斑块
>
> 滚动2次，以帮助CNN模型在
>
> 还有温和的阶级。
>
> 下面的Python代码实现了图像的水平滚动：
>
> 卷\u img[：，0:w-卷\u大小，：]=原始\u img[：，卷_尺寸：w,:]
>
> 横滚，横滚_尺寸：w，：]=raw\u img[：，0:卷大小，：]
>
> 
>
> 2.3.1将分类转换为回归问题。其中一个在这项工作的挑战是，从图像标签皮肤科医生很吵。我们注意到有多个训练图像集中相同（或接近相同）的图像不同的皮肤科医生给它们贴了不同的标签。数字图4显示了由不同用户提供的标签的广泛分布皮肤科医生就几个例子测试图像。图4。测试图像上标签分布广泛考虑到标签的顺序性质，使用回归模型代替建立了分类标准。较高的数值表示更严重的痤疮病变。
>
> 2.3.2迁移学习。迁移学习利用知识从一个领域解决另一个领域的问题。我们用了一个预训练模型从底层提取特征，然后训练一个定制的神经网络模型来解决我们的问题。我们使用预先训练好的深度学习模型从图像中提取特征训练图像的皮肤补丁。考虑到性能和
>
> 计算效率，实验表明ResNet 152由CNTK提供，表现最佳。然后我们训练了一个三层完整的-连接回归神经网络模型（1024，512，和256个隐藏的神经元）使整个针对痤疮严重程度领域的学习模型。我们描述了一种开发基于CNN的传输的实用方法评价痤疮严重程度的学习回归模型自拍图像的损伤。这个模特能在现场表演训练有素的皮肤科医生水平，允许NSH部署消费者应用程序，以协助治疗这种疾病。提出了人脸地标模型与人脸识别的耦合方法一眼OpenCV模型提取皮肤patc

> **相关研究**
>
> http://www.mecs-press.org/ijitcs/ijitcs-v11-n11/v11n11-6.html
>
> A Web-Based Skin Disease Diagnosis Using Convolutional Neural Networks
>
> 基于Web的方法诊断皮肤病，使用CNN来分类，仅停留在分类问题上
>
> 
>
> https://ieeexplore.ieee.org/document/7535331
>
> Multi-Class Skin Diseases Classification Using Deep Convolutional Neural Network and Support Vector Machine
>
> 提出了几种图像分割方法来检测痤疮病变，并使用机器学习方法来区分不同的痤疮病变。
>
> 
>
> https://ieeexplore.ieee.org/document/8280925
>
> Automated detection, extraction and counting of acne lesions for automatic evaluation and tracking of acne severity
>
> 相较上文，运用了不同的斑点检测方法
>
> 
>
> 

目的：不同卷积神经网络算法应用于面部痤疮的分级评判中，研究深度学习辅助评估特定皮肤疾病的最佳思路与方法

关于痤疮严重程度分级标准：温妮——P11

分级法、计数法  各有优缺点	Hayashi  温妮——P15？ 问一下医生什么评级方法 ---》Pillsbury

好的诊断方法：准确、可重复性好、容易被接受、价格低廉、易于操作、对临床决策有效、

评价部位选择：基于VISIA，限定于面部

## 研究目的和意义

借鉴温妮以及自拍照中提到的研究意义

## 研究现状

借鉴温妮以及自拍照中提到的研究现状

以往的生物医学图像分类工作主要集中在使用高分辨率或近景图像，而不是自拍图像。Esteva等人[4]使用129450张高分辨率聚焦图像训练CNN模型，将图像分为良性和恶性皮肤癌。Choi等人[3]以预训练的VGG-19模型为特征提取工具，建立了随机森林迁移学习模型，对10类视网膜图像进行分类。训练数据集包括397张近景和聚焦图像。-----》因此计算机视觉在相关皮肤病领域有极大应用前景 ----》 而在痤疮检测领域，该类研究较少，很大创新突破点   相关研究有 （温妮）

## 

# 中期答辩

1. 数据统计

   1075张正脸照片   痤疮个数，相应种类数

2. baseline

   各种细节信息

   以及各种不足之处，可以做的修改：分割图像、交叉验证、
   
3. yolo

   yolo多尺度，size

> https://blog.csdn.net/bevison/article/details/108396388?utm_medium=distribute.pc_relevant.none-task-blog-baidujs_baidulandingword-4&spm=1001.2101.3001.4242
>
> 一，卫星图目标的尺寸，方向多样。卫星图是从空中拍摄的，因此角度不固定，像船、汽车的方向都可能和常规目标检测算法中的差别较大，因此检测难度大。针对这一点的解决方案是对数据做尺度变换，旋转等数据增强操作。
> 二，小目标的检测难度大。针对这一点解决方案有下面三点。
>
> 1、修改网络结构，使得YOLOV2的stride变成 16 16 16，而不是原始的 32 32 32，这样有利于检测出大小在 16 × 16 − > 32 × 32 16\times 16 -> 32\times 32 16×16−>32×32。
>
> 2、沿用YOLOV2中的passthrough layer，融合不同尺度的特征（ 52 × 52 52\times 52 52×52和 26 × 26 26\times 26 26×26大小的特征），**这种特征融合做法在目前大部分通用目标检测算法中被用来提升对小目标的检测效果**。
>
> 3、不同尺度的检测模型融合，即Ensemble，原因是例如飞机和机场的尺度差异很大，因此采用不同尺度的输入训练检测模型，然后再融合检测结果得到最终输出。
>
> 三，卫星图像尺寸太大。解决方案有将原始图像切块，然后分别输入模型进行检测以及将不同尺度的检测模型进行融合。
>
> ----------------------------------------------------------------------------------------------------------------
>
> 为了提高你的模型在小目标上的性能，我们推荐以下技术：
>
> - 提高图像采集的分辨率
> - 增加模型的输入分辨率
> - tile你的图像
> - 通过增强生成更多数据
> - 自动学习模型anchors
> - 过滤掉无关的类别
>
> --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
>
> 、小目标检测可行的几点方法：
>
> 1、多尺度训练与测试；
>
> 2、数据增强
>
> 3、特征融合模块设计
>
> 4、特征增强模块设计
>
> 5、上下文推理模块设计
>
> 6、膨胀卷积的利用
>
> 7、Anchor free是一个趋势
>
> 8、Focal loss
>
> 9、特征提取骨干网络设计
>
> 10、动态区域放大机制：基于强化学习
>
> 11、超分辨率重建
>
> 12、注意力机制（利用上层语义信息生成注意力）

> 尺度不均：
>
> 前人是如何解决这个多尺度问题的了？1） 使用多尺度特征做检测，2）dilated/deformable convolution用于增加大目标的感受野，3） independent predictions at layers of different resolutions，4）context，5）多尺度训练，6）多尺度检测
> ————————————————
> 版权声明：本文为CSDN博主「O天涯海阁O」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
> 原文链接：https://blog.csdn.net/zhangjunhit/article/details/78834878



问题：痤疮辨别与角度有关

​			侧脸数据集没用上

# 李医生

> [efficientNet]([EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://zhuanlan.zhihu.com/p/96773680/https://arxiv.org/abs/1905.11946))
>
> 卷积神经网络（ConvNets）通常是在固定的资源预算下发展起来的，如果有更多的资源可用的话，则会扩大规模以获得更好的精度，比如可以提高**网络深度(depth)**、**网络宽度(width)**和**输入图像分辨率 (resolution)**大小。但是通过人工去调整 depth, width, resolution  的放大或缩小的很困难的，在计算量受限时有放大哪个缩小哪个，这些都是很难去确定的，换句话说，这样的组合空间太大，人力无法穷举。基于上述背景，该论文提出了一种新的模型缩放方法，它使用一个简单而高效的复合系数来从depth, width, resolution  三个维度放大网络，不会像传统的方法那样任意缩放网络的维度，基于神经结构搜索技术可以获得最优的一组参数(复合系数)。从下图可看出，EfficientNet不仅比别的网络快很多，而且精度也更高。
>
> (a)是基本模型，（b）是增加宽度，（c）是增加深度，（d）是增大属兔图像分辨率，（d）是EfficientNet，它从三个维度均扩大了，但是扩大多少，就是通过作者提出来的**复合模型扩张方法**结合**神经结构搜索技术**获得的

> 数据增强
>
> https://zhuanlan.zhihu.com/p/84604045
>
> https://www.zhihu.com/question/319291048
>
> Data Augmentation | How to use Deep Learning when you have Limited Data
>
> - get_transforms()
>
>   颜色抖动ColorJitter，改变图像的属性：亮度（brightness）、对比度（contrast）、饱和度（saturation）和色调（hue)
>
>   RandomVerticalFlip(),RandomHorizontalFlip()：随机水平垂直翻转
>
>   RandomGaussianBlur()：通常用它来减少 [图像噪声](http://baike.baidu.com/view/944141.htm)以及降低细节层次
>
>   Normalize()：提供一个所有通道的均值（mean） 和方差（std），会将原始数据进行归一化
>
> ~~图像预处理、图像增强、图像平滑？？？~~ ----》 数据增强
>
> 数据增强也叫数据扩增，意思是在不实质性的增加数据的情况下，让有限的数据产生等价于更多数据的价值
>
> 数据增强原因: 
>
> 一般而言，比较成功的神经网络需要大量的参数，许许多多的神经网路的参数都是数以百万计，而使得这些参数可以正确工作则需要大量的数据进行训练，而实际情况中数据并没有我们想象中的那么多
>
> “因为数据不够啊”，确实有时我们获得的数据有限，不足以训练模型，但碍于种种原因（时间，金钱，精力等）我们**不便于去采集大量新的数据**，所以我们想要通过数据增强来获得更大的数据集。但还有其他需要数据增强的原因吗？我认为是有的，可能是想要**增加先验知识或者条件**，或者对数据**增加一些限制和约束**。
>
> 为何行得通：原因之一是**通过数据增强得到的新数据服从实际情况下的分布**，起码也要像上面例子一样，分布与实际情况下尽可能相似。在样本空间上更多的采样点有助于我们进一步探索真实的数据分布。
>
> 另一个原因就是**数据增强能够按照我们的意愿对数据做一些约束**。如果我想使分类网络基于形状进行判断而不是颜色，那么我可以把数据集的颜色信息都去除。因为“网络不一定能学习到数据中对任务有用的全部信息”。
>
> 而数据增强是如何提高模型的正则化效果的？数据增强在某方面使得模型更集中地观测那些数据总的普遍模式，而消除了某些和普遍模式无关的数据。

> K折交叉
>
> https://blog.csdn.net/weixin_39562554/article/details/110804625

使用EfficientNet-B3作为骨干网络，图像输入大小为300*300。随机水平垂直翻转，高斯滤波器进行平滑处理，图像归一化处理。所有标注图像的80%作为训练集，训练过程采用5折交叉验证，验证集用于开发过程中评估模型性能和超参数。剩余所有标注图像的20%作为测试集，用于生成最终性能指标。（亟待丰富）



使用EfficientNet-B3作为骨干网络。EfficientNet相较于传统的卷积神经网络的优势在于，传统神经网络需要人工调整网络深度、网络宽度和输入图像分辨率这三个维度来获得更好的精度，然而在计算量受限时很难确定去放大或者缩小哪一个维度，而EfficientNet使用了复合扩展方法，用一个简单而高效的复合系数来统一扩展所有维度，并基于神经结构搜索技术来获得最优的一组复合系数。因此EfficientNet不仅比别的网络快很多，而且精度也更高。在数据集处理方面，所有标注图像的80%作为训练集，剩余所有标注图像的20%作为测试集。其中，训练集中的图像会采用数据增强技术进行处理，通过对图像随机水平垂直翻转，使用高斯滤波器进行平滑处理，图像归一化处理，从而在不实质性增加数据的情况下，让有限的数据产生等价于更多数据的价值。并采用5折交叉验证来划分训练集，即将训练集分成5组，将每个子集数据分别做一次验证集，其余的4组子集数据作为训练集，这样会得到5个模型，评估 5 个模型的效果，从中挑选效果最好的超参数，使用最优的超参数，然后将 5 份数据全部作为训练集重新训练模型，得到最终模型，最后用测试集评估模型生成最终性能指标。



EfficientNet

https://zhuanlan.zhihu.com/p/96773680

论文：EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks



数据增强

https://zhuanlan.zhihu.com/p/84604045

https://www.zhihu.com/question/319291048

论文：Data Augmentation | How to use Deep Learning when you have Limited Data



K-折交叉验证

https://blog.csdn.net/weixin_39562554/article/details/110804625



# 毕设论文

> 首先，是快速大改，把论文分段复制以后，翻译成英文，再翻译成中文，能改一半左右查重，然后需要把不通顺的句子改一下，不然会有语句不通。针对一大部分论文都有效果，也存在翻译之后，没有变化的例子，但多数都能改很多。
>
> 10%-20%之间的，这其实已经符合了很多学校的要求，但是也有些学校要求是10以下，一般针对这种论文，主要就是倒装句、改变句子语序、增加词语、增加句子长度等，来避开查重。
>
> 一般10%以下就符合95%以上学校了，改查重无非就是替换词语、改变语序、增加词语、删减句子等。具体实施可以自己摸索以下，或者是问我都可以。
>
> 
>
> 

## 课题背景及目的

​		痤疮是毛囊皮脂腺单位的一种多因素疾病。临床表现从轻度的粉刺型痤疮到暴发型伴有系统性症状的痤疮，主要表现形态有粉刺、丘疹、脓疱、结节、囊肿，等等。痤疮通常被认为是青少年的一种疾病，越85%的12-24岁的青少年患此病。尽管寻常痤疮大多见于青少年人群，但是有大约3%的男性和12%的女性会患病持续到44岁。痤疮往往会改变患者的容貌外观，这种改变经常会造成患者的自卑情绪甚至上升到社会隔离，而不规范的治疗手段往往会加重患者面部痤疮严重程度，更加会对患者的身心健康造成严重并且不可逆的伤害。同时，痤疮在经济上的影响也是无可否认的。例如在美国，每年会花费超过25亿美元在治疗痤疮上，这也是对医疗资源的沉重负担。因此，及时规范的痤疮治疗是十分有必要的。

​		传统的痤疮治疗，需要先由皮肤科医生在临床环境中对患者的痤疮严重程度进行评估，再由医生进行处方治疗或根据严重程度推荐非处方护肤品。在这之中，对痤疮严重程度准确评级是及时规范的痤疮治疗的基础。痤疮严重程度的分级标准是不完全统一的，其中包括国际改良分类法、GAGS分类法、Pillsbury分类法、MDI分类法、Hayashi分类法，这些分类法依靠的主要是简单计数皮损个数以及皮损的种类来划分痤疮的严重程度，例如Hayashi分类法将炎性皮损数少于6个分为轻度，炎性皮损数在6-20个分为中度，炎性皮损数在21-50个分为重度，炎性皮损数大于60个分为极重度。因此皮损个数的精确计数在对痤疮严重程度评级上起了决定性的作用。

​		VISIA皮肤检测仪是一种能对皮肤的病理学特征进行定量分析的仪器。它是皮肤检测仪器的一个重要里程碑，使用超高清的摄像头，并通过白光、紫外光、偏振光三种光源扫描皮肤表层和深层，从而检测出斑点以及毛孔、皱纹、皮肤纹理、皮肤细菌状态等信息。依靠VISA皮肤检测仪中的白光拍摄，可以得到患者面部痤疮的详细信息，清晰地展现痤疮的位置、种类、分布等信息，对开展皮肤痤疮研究工作有着重要意义。

​		本项目旨在使用深度学习方法，搭建自动检测人脸痤疮位置以及数目的模型，并使用VISIA皮肤检测仪收集到的数据来训练和评估模型。本项目构建的模型可以给患者以及低年资的医生提供客观的、可靠的、快速的评估意见，帮助其进行痤疮分级评估，从而采取更加科学准确的治疗方法，减少不规范的治疗，这对患者以及整个社会都有重大的意义。

## 国内外研究现状

### 传统机器学习算法

在图像视觉领域中，传统的机器学习算法模型主要分为5个步骤，分别为特征感知、图像预处理、特征提取、特征筛选、推理预测与识别。而对于传统目标检测任务，它通常需要选择检测窗口、提取图像特征、设计分类器这三个步骤，并且往往所提取的特征主要围绕底层特征和中层次特征来展开，例如颜色、纹理等。曾经出现过许多优秀的检测算法，例如有Viola-Jones人脸检测器主要用于人脸的检测，有向梯度直方图(Histogram of Oriented Gradients，HOG)与支持向量机（Support Vector Machine，SVM）算法的出现被应用于行人检测，以及可变型部件模型算法（Deformable Part Model，DPM）在深度学习方法成熟之前一直在目标检测中发挥作用。

传统的机器学习方法也有被用于皮肤痤疮的检测。于2016年Nasim Alamdari提出了一种面部皮肤缺陷自动检测与识别系统，该系统提出了几种图像分割方法来检测痤疮病灶位置和几种机器学习方法来区分不同的痤疮病灶的类别。论文使用了纹理分析、k均值聚类、HSV模型分割技术来检测痤疮病灶位置。其中k均值聚类法优于其它几个方法，检测准确率达到大约70%。而在痤疮病灶的分类问题中，论文使用了模糊C均值聚类算法和支持向量机方法，在鉴别痤疮疤痕和活动性炎症性病变问题中分别取得80%和66.6%的准确率，同时模糊C均值聚类算法在鉴别正常皮肤和病变皮肤的问题上准确率达到了100%。然而，本文提出的方法依赖于手工设计的特征来完成痤疮的检测与分类任务。同时，本文使用了有限数量的图像来评估分割和分类任务，评估方法仅仅采用35幅图像来检测分割和分类精度，这是无法对该论文提出的方法做到全面评估的，同时也不能保证方法的鲁棒性，因此需要获得更多的图像来检测该系统检测不同病灶的实用性。另外，所需要的图片需要有特定的规定，所有图像大小相同(512 * 512)、8位灰度分辨率和24位每像素的分辨率，如果图像有其它结构的存在（例如头发或）则会大大降低检测的准确性，整个系统的鲁棒性存在着比较大的问题。

尽管传统的机器学习方法在某些具体任务中有不错的表现，然而针对每个具体的任务，由于传统方法依赖于手工设计的特征，都需要研究人员有丰富的经验，才能设计出合适的特征算法，此外还需要合适的分类器，才能取得预期的效果，而这些条件相对来说是非常苛刻的，因此也很难达到最优的效果。

### 深度学习算法

近些年来，深度学习（Deep Learning）理论被广泛应用于图像视觉领域，而其中卷积神经网络是应用最广泛的一种神经网络结构，它在1986年由LeCun提出。在早期卷积神经网络被成功用于手写字符图像识别，而在2012年AlexNet这个更深层次的神经网络结构取得成功后，卷积神经网络蓬勃发展，被广泛用于各个领域。而在视觉上，包括图像分类、图像检测、图像分割等问题卷积神经网络都有所应用。

而医学疾病的诊断也越来越受到图像视觉研究领域人员的关注。在许多计算机视觉任务上取得良好性能的深度学习方法也被用于医学影像领域。在2017年斯坦福大学在Nature杂志上发表了一篇研究，它们使用卷积神经网络来鉴别恶性黑色素瘤与良性色素痣、角质形成细胞癌与良性脂溢性角质化，在这两个任务中神经网络的诊断准确率与21位皮肤科专科医生相媲美。也有研究表明，除了皮肤镜图像以外，卷积神经网络在大类别皮肤病的普通数码照片样本中也具有良好的分类效果。深度学习方法在医学领域有着广泛的应用前景。

深度学习方法在皮肤痤疮诊断中也有所应用。在2019年Nestlé SHIELD [3](Skin Health, Innovation, Education, and Longevity Development, NSH)与微软合作开发了一款用于痤疮评估的消费者移动应用程序，它主要根据用户上传的自拍图像评估用户的痤疮严重程度。在数据集上，NSH为这项研究获得了4700张自拍照片，从中剔除曝光过度或曝光不足以及分辨率非常低的图片，并分配给11名皮肤科医生来标注其痤疮严重程度。在数据预处理方面，NSH使用OpenCV检测面部标志，从自拍照中剪切了特定的皮肤贴片（额头、左右脸颊、下巴），并且为了解决卷积神经网络在空间上敏感的问题，采用滚动图片贴片方法将每个皮肤贴片滚动到一定数量的像素上。然后使用ResNet 152[10]从皮肤贴片中提取特征，再输入到一个训练好的3层全连接回归神经网络模型（有1024、512和256个神经元）中，最后输出痤疮严重程度。该基于卷积神经网络的回归模型在区分轻度痤疮的图像上表现良好，但在区分其它几种痤疮严重程度上表现仍然令人不满意。该模型在采用滚动图片贴片方法时引入了额外的噪声，会对检测结果产生影像；该模型的数据集为自拍照片，照片质量层次不齐，照片数目也相对较少，对模型的训练结果也会有较大影响；另外，模型直接对输入的图像判定痤疮严重程度，而未能检测痤疮的位置以及数目等细节信息。

在2019年由Xiaoping Wu、Ni Wen、Jie Liang等人提出了一个统一的深度框架，该框架有两个分支完成痤疮病变计数和痤疮严重程度分级两个任务，先由计数分支预测痤疮病变的分布情况，然后映射到痤疮的严重程度分布上，再由分级分支结合预测的痤疮严重程度分布和映射的严重程度分布来完成对痤疮的分级任务。在计数分支中，论文使用了KL loss来最小化实际目标框和预测目标框的偏差，并分别使用了基于检测和回归的技术方法并进行了比较，例如基于检测的Faster-RCNN方法的效果通常优于基于回归的方法，其达到的最佳准确率高达73.97%，然而其在非常严重等级的痤疮样本中的准确率仅仅只有4.62%，说明Faster-RCNN在每个痤疮类别中很难达到平衡的结果。此外，他们还构建了ACNE04数据集，该数据集是用数码相机收集痤疮病变的图像，所有图像都是从患者的正面以大约70度的角度拍摄所得。数据集包含1457张图像，其中标注了18983个病变的边界框，所有图像都经过专家对皮损数量和痤疮严重程度进行统计和标注。该数据集对后序的痤疮研究工作也有很大的贡献。

### 研究内容

​		本文研究的主要问题是搭建自动检测痤疮模型，依据输入的图像自动标注痤疮所在位置。以VISIA皮肤检测仪收集到的图像作为主要数据集，该图像集为高清图像集，包含不同痤疮严重程度的患者正脸以及侧脸图像。需要对该图像集做出分析，分析痤疮数量、痤疮尺度、所处位置、密集程度等因素，挑选合适的深度学习方法和自动检测框架，并采取合适的改进方案，进一步提升自动检测痤疮模型的检测精度。

​		本文关于搭建自动痤疮模型的具体研究内容如下：

​		1）使用VISIA皮肤检测仪采集不同痤疮严重程度的患者的正脸与侧脸图像，并完成痤疮位置标注工作。针对该数据集，分析痤疮的尺度大小以及相对整体图像的尺度比例、分析痤疮的密集程度和数量等因素，把握图像特点以及需要检测的痤疮目标的特点，并对该图像集进行合理的划分，划分为训练集、测试集、验证集。

​		2）根据图像分析所得到的信息，选取合适的目标检测模型作为基础模型，并结合该模型的特点和图像集信息的特点，在数据增强、训练尺度、锚点框尺寸、骨干网络等方面改进目标检测模型，进一步提升检测精度。

​		3）针对改进前与改进后的目标检测模型，使用由VISIA皮肤检测仪收集的图像划分出的测试集，检测模型精确度和其它各项指标的提升程度；针对最终的目标检测模型，使用别的不同类型的数据集，检测模型在不同数据集中的检测能力。

## 相关原理与技术

### YOLOV3检测算法

- Yolov3网络架构

  ​		darknet-53是yolov3算法的主干网络，yolov3使用了darknet-53的前52层（无全连接层）。yolov3是一个全卷积网络，它摒弃了池化层来降低池化所带来的梯度负面效果，而是通过调节卷积步长来实现下采样，控制输出特征图的尺度大小，也因此yolov3对于输入图片的尺寸大小并没有特别的限制。同时，为了加强算法对小目标的检测精度，yolov3借鉴FPN（feature pyramid networks）的思想，即同时利用低层特征具有高分辨率的特性和高层特征具有高语义信息的特性，通过融合这些低层和高层不同层次的特征信息来达到预测的效果。yolov3的第一张特征图下采样32倍，第二张特征图下采样16倍，第三张特征图下采样8倍，其中小尺度特征图用于检测大尺寸物体，大尺度特征图用于检测小尺寸物体，通过融合这三张不同尺度的特征图来增强检测效果。此外，yolov3结构中还融合了Resnet和Densenet的结构和技巧：使用了concat张量拼接操作，该操作源于DenseNet网络的设计思路，将特征图按照通道维数直接进行拼接操作，从而会扩充张量的维度；引入残差层结构，该结构源于ResNet思想，使用加和操作，即将输入特征层和输出特征层对应维度直接相加而非拼接。

- Yolo输出特征图过程

  ​		yolov3输出的结果就是三张特征图。特征图大小为N * N，其中每个单元格都有3个不同尺度的先验框，每个先验框需要4维来确定位置，需要1维表示检测置信度，K维表示类别，因此每个特征图的输出维度为 N * N *(3 * (4 + 1 + K))。

  - 先验框与检测框

    ​		在yolov1中，神经网络直接回归输出检测框的宽、高，然而这种方法精度有限且训练较慢。在yolov2和yolov3中就采用先验框，降低了网络的学习难度，大大提升了整体的检测精度。一共有9个先验框，在coco数据集中分别为 (10×13)，(16×30)，(33×23)，(30×61)，(62×45)，(59× 119)， (116 × 90)， (156 × 198)，(373 × 326) ，顺序为w × h。从前往后每三个先验框为一组，分别归属于三张输出特征图，

    ​		得到输出特征图的输出向量以及先验框预先设定的尺寸，就可以得到检测框的位置。检测框的x，y，w，h计算公式如下：
    $$
    b_x = \sigma(t_x) + c_x	\\
    b_y = \sigma(t_y) + c_y	\\
    b_w = p_we^{t_w}	\\
    b_h = p_he^{t_h}
    $$
    

    其中c_x、c_y为cell左上角坐标，t_x、t_y、t_w、t_h为特征图输出中关于先验框信息的4维向量，p_w、p_h为先验框预先设定的宽和高，b_x、b_y、b_w、b_h为检测框的x、y、w、h，\sigma为激活函数，在yolov3中使用的是sigmoid函数。

  - 置信度

    每个先验框都需要一维表示置信度。置信度包含两重信息：第一重信息是当前预测框是否包含对象的概率，即判定当前预测框内是物体还是背景；第二重信息是当前预测框若有物体，则预测框和物体真实框可能的IOU(Intersection over Union)交互比。关于置信度的计算公式如下：

    ​	Cij=Pr(Object)∗IOUpredtruth

    其中，Pr表示预测该预测框中是否存在物体，存在为1，不存在则为0，IOU则为预测框和物体真实框的交互比，置信度则为这二者的乘积。

    







