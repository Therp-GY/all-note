# SVM

![1574608741079](C:\Users\GY\AppData\Roaming\Typora\typora-user-images\1574608741079.png)

拉格朗日法是一定适合于凸问题的，不一定适合于其他问题

既然有了约束不能直接求导，那么如果把约束去掉不就可以了吗？怎么去掉呢？这才需要拉格朗日方法。既然是等式约束，那么我们把这个约束乘一个系数加到目标函数中去，这样就相当于既考虑了原目标函数，也考虑了约束条件，比如上面那个函数，加进去就变为：
$$
f = 2x_1 ^2 + 3x_2^2+7x_3^2+\alpha_1(2x_1+x_2-1)+\alpha_2(2x_2+3x_3-2)
$$
这里可以看到与相乘的部分都为0，所以的取值为全体实数。现在这个优化目标函数就没有约束条件了吧，既然如此，求法就简单了，分别对x求导等于0，如下：

![1574608793009](C:\Users\GY\AppData\Roaming\Typora\typora-user-images\1574608793009.png)





```python
from numpy import *
 
 
class Model(object):
 
    def __init__(self, X, y, C, toler, kernel_param):
        self.X = X
        self.y = y
        self.C = C
        self.toler = toler
        self.kernel_param = kernel_param
        self.m = shape(X)[0]
        #输出X的行数值
        self.mapped_data = mat(zeros((self.m, self.m)))
        #建矩阵
        for i in range(self.m):
            self.mapped_data[:, i] = gaussian_kernel(self.X, X[i, :], self.kernel_param)
        #切片，[:,1]表shi'zhong
        self.E = mat(zeros((self.m, 2)))
        self.alphas = mat(zeros((self.m, 1)))
        self.b = 0
 
 
def load_data(filename):
    X = []
    y = []
    with open(filename, 'r') as fd:
        for line in fd.readlines():
            nums = line.strip().split(',')
            X_temp = []
            for i in range(len(nums)):
                if i == len(nums) - 1:
                    y.append(float(nums[i]))
                else:
                    X_temp.append(float(nums[i]))
            X.append(X_temp)
    return mat(X), mat(y)
 
def gaussian_kernel(X, l, kernel_param): 
    sigma = kernel_param 
    m = shape(X)[0]
    mapped_data = mat(zeros((m, 1)))
    for i in range(m):
        mapped_data[i] = exp(-sum((X[i, :] - l).T * (X[i, :] - l) / (2 * sigma ** 2)))
    return mapped_data
 
def clip_alpha(L, H, alpha):
    if alpha > H:
        alpha = H
    elif alpha < L:
        alpha = L
    return alpha
 
def calc_b(b1, b2):
    return (b1 + b2) / 2
 
def calc_E(i, model):
    yi = float(model.y[i])
    gxi = float(multiply(model.alphas, model.y).T * model.mapped_data[:, i] + model.b)
    Ei = gxi - yi
    return Ei
 
def select_j(Ei, i, model):
    nonzero_indices = nonzero(model.E[:, 0].A)[0]
    Ej = 0
    j = 0
    max_delta = 0
    if len(nonzero_indices) > 1:
        for index in nonzero_indices:
            if index == i:
                continue
            E_temp = calc_E(index, model)
            delta = abs(E_temp - Ei)
            if delta > max_delta:
                max_delta = delta
                Ej = E_temp
                j = index
    else:
        j = i
        while j == i:
            j = int(random.uniform(0, model.m))
        Ej = calc_E(j, model)
    return j, Ej
 
def iterate(i, model):
    yi = model.y[i]
    Ei = calc_E(i, model)
    model.E[i] = [1, Ei]
    # 如果alpahi不满足KKT条件, 则进行之后的操作, 选择alphaj, 更新alphai与alphaj, 还有b
    if (yi * Ei > model.toler and model.alphas[i] > 0) or (yi * Ei < -model.toler and model.alphas[i] < model.C):
        # alphai不满足KKT条件
        # 选择alphaj
        j, Ej = select_j(Ei, i, model)
        yj = model.y[j] 
        alpha1old = model.alphas[i].copy()
        alpha2old = model.alphas[j].copy()
        eta = model.mapped_data[i, i] + model.mapped_data[j, j] - 2 * model.mapped_data[i, j]   
        if eta <= 0:
            return 0
        alpha2new_unclip = alpha2old + yj * (Ei - Ej) / eta
        if yi == yj:
            L = max(0, alpha2old + alpha1old - model.C)
            H = min(model.C, alpha1old + alpha2old)
        else:
            L = max(0, alpha2old - alpha1old)
            H = min(model.C, model.C - alpha1old + alpha2old)
        if L == H:
            return 0
        alpha2new = clip_alpha(L, H, alpha2new_unclip)
        if abs(alpha2new - alpha2old) < 0.00001:
           return 0
        alpha1new = alpha1old + yi * yj * (alpha2old - alpha2new)
        b1new = -Ei - yi * model.mapped_data[i, i] * (alpha1new - alpha1old) \
                - yj * model.mapped_data[j, i] * (alpha2new - alpha2old) + model.b
        b2new = -Ej - yi * model.mapped_data[i, j] * (alpha1new - alpha1old) \
                - yj * model.mapped_data[j, j] * (alpha2new - alpha2old) + model.b
        model.b = calc_b(b1new, b2new)
        model.alphas[i] = alpha1new
        model.alphas[j] = alpha2new
        model.E[i] = [1, calc_E(i, model)]
        model.E[j] = [1, calc_E(j, model)]
        return 1
    return 0
 
def smo(X, y, C, toler, iter_num, kernel_param):
    model = Model(X, y.T, C, toler, kernel_param)
    changed_alphas = 0
    current_iter = 0
    for i in range(model.m):
        changed_alphas += iterate(i, model)
        print("iter:%d i:%d,pairs changed %d"
              %(current_iter, i, changed_alphas))
    current_iter += 1
    print('start...') 
    while current_iter < iter_num and changed_alphas > 0:
        changed_alphas = 0
        # 处理支持向量
        alphas_indice = nonzero((model.alphas.A > 0) * (model.alphas.A < C))[0]
        for i in alphas_indice:
            changed_alphas += iterate(i, model)
            print("iter:%d i:%d,pairs changed %d"
                  %(current_iter, i, changed_alphas))
        current_iter += 1

————————————————
版权声明：本文为CSDN博主「MegaC」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/qq_41290282/article/details/88326559
```

## SMO

最后转化成一下的问题，\alpha为乘子，有0-C的约束条件，x、y为每个点的坐标

![1574656282261](C:\Users\GY\AppData\Roaming\Typora\typora-user-images\1574656282261.png)

我们要做的就是调整\alpha值来求W的最小值

1. 那么应该找哪些\alpha呢？**找不满足kkt规则的**

满足：

(1)点在分界线之外

(2)点在分界线上

(3)点在两条线间，但是满足稀疏值

![1574656840964](C:\Users\GY\AppData\Roaming\Typora\typora-user-images\1574656840964.png)

不满足：

![1574656856618](C:\Users\GY\AppData\Roaming\Typora\typora-user-images\1574656856618.png)

2.找到\alpha如何更改	SMO ---》求出新的w和b，求着求着就得到最终的w和b，它们是可以用\alpha算出来的





# 随机森林

决策树关键
经验熵 特征的条件熵 两者相减得到信息增益 
靠信息增益确定特征所在决策树的节点位置

连续变量找中点 划分大于他和小于他的为两类从而离散化，要对每个中间求条件熵得到最大增益率

cart 不用信息增益 而用不纯度作为选择依据




随机森林 创造很多棵决策树 统计他们的结果最后得到结果 生成方式不算复杂


//////////////////////////////

- 经验熵： P对应的是你要分的类别种类，即对样本结果集合的概率计算

- 条件熵： 依靠条件特征可以将特征类别分为i类
  每个为Di ，每个条件下结果有个比率，靠Di占总D的比率和其对应结果比率可以算条件熵

- 增益率计算：关键在于SplitInfo计算 他是对自己每个Di做熵计算，而无关分类值

- 训练的时候是每经过一个节点 样本由节点一分为二然后继续分，样本是会被分开的
  预测的时候数据进来是一条一条预测的，走进来最后被分到一个树叶上

- 同一属性可能会出现在多个节点上，并以不同value值出现，相当于多层细分



# pandas

- iloc loc

  loc：通过行号截取数据

  iloc：通过行标签截取数据

  `df.iloc[:count, :-1]`：表示从第0行到count行，截取属性1

- isin

  进行过滤。对

  df[df['label'].isin([1,2])]  过滤'label'列不为1或2的数据，将这些行剔除

- sample

  对df进行随机抽样

  frac参数：抽取行比例

  random_state:随机数发生器种子， =1 可重复， = None 不可重复

- reset_index

  重新标索引

- concat

  df拼接操作

  axis参数：=0 纵向拼接， =1 横向拼接

- to_frame

  将数组转化为dataframe操作

  name参数：name = "val" 将列名取为val

- .columns

  返回一个列的索引，显示所有列属性

- .unique

  去除数组或者列表重复元素并返回由大到小新元组或列表

# else

- random.sample(list, 5)

  从list中随机选5个数

  `random_state_stages = random.sample(range(self.n_estimators), self.n_estimators)`这句话是否仅仅打乱顺序

- python range(a)

  以a创造一个整数列表

- python  .sort()

  对原列表进行排序

- collections.Counter(list)

  将list中相同元素变成字典中一个key，value对应出现次数
  
- for _, 

  占位符，表示虽然要n循环但是不需要i